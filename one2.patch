diff --git a/compiler/loco/include/loco/IR/NodeMixins.h b/compiler/loco/include/loco/IR/NodeMixins.h
index f0e34b0ba..e98a9bdee 100644
--- a/compiler/loco/include/loco/IR/NodeMixins.h
+++ b/compiler/loco/include/loco/IR/NodeMixins.h
@@ -99,7 +99,7 @@ template <unsigned N> struct FixedArity
     virtual ~Mixin() = default;
 
   public:
-    unsigned arity(void) const final { return N; }
+      uint32_t arity(void) const final { return N; }
 
     Node *arg(uint32_t n) const final { return _args.at(n)->node(); }
 
diff --git a/compiler/luci-interpreter/src/CMakeLists.txt b/compiler/luci-interpreter/src/CMakeLists.txt
index 6e1d600f9..df3d20520 100644
--- a/compiler/luci-interpreter/src/CMakeLists.txt
+++ b/compiler/luci-interpreter/src/CMakeLists.txt
@@ -1,27 +1,27 @@
-nnas_find_package(TensorFlowSource EXACT 2.3.0 QUIET)
-nnas_find_package(TensorFlowGEMMLowpSource EXACT 2.3.0 QUIET)
-nnas_find_package(TensorFlowEigenSource EXACT 2.3.0 QUIET)
-nnas_find_package(TensorFlowRuySource EXACT 2.3.0 QUIET)
-
-if (NOT TensorFlowSource_FOUND)
-  message(STATUS "Skipping luci-interpreter: TensorFlow not found")
-  return()
-endif ()
-
-if (NOT TensorFlowGEMMLowpSource_FOUND)
-  message(STATUS "Skipping luci-interpreter: gemmlowp not found")
-  return()
-endif ()
-
-if (NOT TensorFlowEigenSource_FOUND)
-  message(STATUS "Skipping luci-interpreter: Eigen not found")
-  return()
-endif ()
-
-if (NOT TensorFlowRuySource_FOUND)
-  message(STATUS "Skipping luci-interpreter: Ruy not found")
-  return()
-endif ()
+#nnas_find_package(TensorFlowSource EXACT 2.3.0 QUIET)
+#nnas_find_package(TensorFlowGEMMLowpSource EXACT 2.3.0 QUIET)
+#nnas_find_package(TensorFlowEigenSource EXACT 2.3.0 QUIET)
+#nnas_find_package(TensorFlowRuySource EXACT 2.3.0 QUIET)
+#
+#if (NOT TensorFlowSource_FOUND)
+#  message(STATUS "Skipping luci-interpreter: TensorFlow not found")
+#  return()
+#endif ()
+#
+#if (NOT TensorFlowGEMMLowpSource_FOUND)
+#  message(STATUS "Skipping luci-interpreter: gemmlowp not found")
+#  return()
+#endif ()
+#
+#if (NOT TensorFlowEigenSource_FOUND)
+#  message(STATUS "Skipping luci-interpreter: Eigen not found")
+#  return()
+#endif ()
+#
+#if (NOT TensorFlowRuySource_FOUND)
+#  message(STATUS "Skipping luci-interpreter: Ruy not found")
+#  return()
+#endif ()
 
 add_subdirectory(core)
 add_subdirectory(kernels)
@@ -31,7 +31,7 @@ set(SOURCES
     "${LUCI_INTERPRETER_INCLUDE_DIR}/luci_interpreter/Interpreter.h"
     Interpreter.cpp)
 
-add_library(luci_interpreter SHARED ${SOURCES})
+add_library(luci_interpreter STATIC ${SOURCES})
 target_include_directories(luci_interpreter PUBLIC "${LUCI_INTERPRETER_INCLUDE_DIR}")
 target_include_directories(luci_interpreter PRIVATE "${LUCI_INTERPRETER_SOURCE_DIR}")
 target_link_libraries(luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/BatchToSpaceND.test.cpp b/compiler/luci-interpreter/src/kernels/BatchToSpaceND.test.cpp
deleted file mode 100644
index a29981d17..000000000
--- a/compiler/luci-interpreter/src/kernels/BatchToSpaceND.test.cpp
+++ /dev/null
@@ -1,92 +0,0 @@
-/*
- * Copyright (c) 2021 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/BatchToSpaceND.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-template <typename T>
-void Check(std::initializer_list<int32_t> input_shape,
-           std::initializer_list<int32_t> block_shape_shape,
-           std::initializer_list<int32_t> crops_shape, std::initializer_list<int32_t> output_shape,
-           std::initializer_list<T> input_data, std::initializer_list<int32_t> block_shape_data,
-           std::initializer_list<int32_t> crops_data, std::initializer_list<T> output_data)
-{
-  constexpr DataType element_type = getElementType<T>();
-  Tensor input_tensor = makeInputTensor<element_type>(input_shape, input_data);
-  Tensor block_shape_tensor = makeInputTensor<DataType::S32>(block_shape_shape, block_shape_data);
-  Tensor crops_tensor = makeInputTensor<DataType::S32>(crops_shape, crops_data);
-  Tensor output_tensor = makeOutputTensor(element_type);
-
-  BatchToSpaceND kernel(&input_tensor, &block_shape_tensor, &crops_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<T>(output_tensor), ::testing::ElementsAreArray(output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), output_shape);
-}
-
-template <typename T> class BatchToSpaceNDTest : public ::testing::Test
-{
-};
-
-using DataTypes = ::testing::Types<float, uint8_t>;
-TYPED_TEST_CASE(BatchToSpaceNDTest, DataTypes);
-
-TYPED_TEST(BatchToSpaceNDTest, Simple)
-{
-  Check<TypeParam>(/*input_shape=*/{4, 2, 2, 1}, /*block_shape_shape=*/{2}, /*crops_shape=*/{2, 2},
-                   /*output_shape=*/{1, 4, 4, 1},
-                   /*input_data=*/{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16},
-                   /*block_shape_data=*/{2, 2}, /*crops_data=*/{0, 0, 0, 0},
-                   /*output_data=*/{1, 5, 2, 6, 9, 13, 10, 14, 3, 7, 4, 8, 11, 15, 12, 16});
-}
-
-TEST(BatchToSpaceNDTest, Invalid_Shape_NEG)
-{
-  Tensor input_tensor =
-    makeInputTensor<DataType::FLOAT32>({3, 2, 2, 1}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12});
-  Tensor block_shape_tensor = makeInputTensor<DataType::S32>({2}, {2, 2});
-  Tensor crops_tensor = makeInputTensor<DataType::S32>({2, 2}, {0, 0, 0, 0});
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  BatchToSpaceND kernel(&input_tensor, &block_shape_tensor, &crops_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(BatchToSpaceNDTest, Invalid_Crops_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(
-    {4, 2, 2, 1}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16});
-  Tensor block_shape_tensor = makeInputTensor<DataType::S32>({2}, {2, 2});
-  Tensor crops_tensor = makeInputTensor<DataType::S32>({2, 2}, {0, 0, -1, 0});
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  BatchToSpaceND kernel(&input_tensor, &block_shape_tensor, &crops_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/CMakeLists.txt b/compiler/luci-interpreter/src/kernels/CMakeLists.txt
index 3b884ddd2..72643da22 100644
--- a/compiler/luci-interpreter/src/kernels/CMakeLists.txt
+++ b/compiler/luci-interpreter/src/kernels/CMakeLists.txt
@@ -1,227 +1,5 @@
-set(SOURCES
-    Add.h
-    Add.cpp
-#    ArgMax.h
-#    ArgMax.cpp
-#    AveragePool2D.h
-#    AveragePool2D.cpp
-#    BatchToSpaceND.h
-#    BatchToSpaceND.cpp
-#    Concatenation.h
-#    Concatenation.cpp
-#    Conv2D.h
-#    Conv2D.cpp
-#    DepthToSpace.h
-#    DepthToSpace.cpp
-#    DepthwiseConv2D.h
-#    DepthwiseConv2D.cpp
-#    Div.h
-#    Div.cpp
-#    Elu.h
-#    Elu.cpp
-#    Exp.h
-#    Exp.cpp
-#    Floor.h
-#    Floor.cpp
-#    FloorDiv.h
-#    FloorDiv.cpp
-#    Equal.h
-#    Equal.cpp
-#    FullyConnected.h
-#    FullyConnected.cpp
-#    Greater.h
-#    Greater.cpp
-#    GreaterEqual.h
-#    GreaterEqual.cpp
-#    If.h
-#    If.cpp
-#    InstanceNorm.h
-#    InstanceNorm.cpp
-#    L2Normalize.h
-#    L2Normalize.cpp
-#    L2Pool2D.h
-#    L2Pool2D.cpp
-#    LeakyRelu.h
-#    LeakyRelu.cpp
-#    Less.h
-#    Less.cpp
-#    LessEqual.h
-#    LessEqual.cpp
-#    LocalResponseNormalization.h
-#    LocalResponseNormalization.cpp
-#    LogicalAnd.h
-#    LogicalAnd.cpp
-#    LogicalNot.h
-#    LogicalNot.cpp
-#    LogicalOr.h
-#    LogicalOr.cpp
-#    Logistic.h
-#    Logistic.cpp
-#    LogSoftmax.h
-#    LogSoftmax.cpp
-#    Maximum.h
-#    Maximum.cpp
-#    MaxPool2D.h
-#    MaxPool2D.cpp
-#    Mean.h
-#    Mean.cpp
-#    Minimum.h
-#    Minimum.cpp
-#    MirrorPad.h
-#    MirrorPad.cpp
-#    Mul.h
-#    Mul.cpp
-#    Neg.h
-#    Neg.cpp
-#    NotEqual.h
-#    NotEqual.cpp
-#    Pack.h
-#    Pack.cpp
-#    Pad.h
-#    Pad.cpp
-#    PadV2.h
-#    PadV2.cpp
-#    Pow.h
-#    Pow.cpp
-#    Prelu.h
-#    Prelu.cpp
-#    Relu.h
-#    Relu.cpp
-#    Relu6.h
-#    Relu6.cpp
-#    Reshape.h
-#    Reshape.cpp
-#    ResizeBilinear.h
-#    ResizeBilinear.cpp
-#    ResizeNearestNeighbor.h
-#    ResizeNearestNeighbor.cpp
-#    Reverse.h
-#    Reverse.cpp
-#    Rsqrt.h
-#    Rsqrt.cpp
-#    Slice.h
-#    Slice.cpp
-#    Softmax.h
-#    Softmax.cpp
-#    SpaceToBatchND.h
-#    SpaceToBatchND.cpp
-#    SpaceToDepth.h
-#    SpaceToDepth.cpp
-#    Split.h
-#    Split.cpp
-#    StridedSlice.h
-#    StridedSlice.cpp
-#    Sqrt.h
-#    Sqrt.cpp
-#    Square.h
-#    Square.cpp
-#    SquaredDifference.h
-#    SquaredDifference.cpp
-#    Squeeze.h
-#    Squeeze.cpp
-#    Sub.h
-#    Sub.cpp
-#    Tanh.h
-#    Tanh.cpp
-#    Transpose.h
-#    Transpose.cpp
-#    TransposeConv.h
-#    TransposeConv.cpp
-#    Unpack.h
-#    Unpack.cpp
-        )
-
-list(APPEND SOURCES
-    BinaryOpCommon.h
-    Utils.h
-    Utils.cpp
-    ${TensorFlowSource_DIR}/tensorflow/lite/kernels/internal/quantization_util.cc)
-
-add_library(luci_interpreter_kernels STATIC ${SOURCES})
-set_target_properties(luci_interpreter_kernels PROPERTIES POSITION_INDEPENDENT_CODE ON)
-target_include_directories(luci_interpreter_kernels PUBLIC ${LUCI_INTERPRETER_SOURCE_DIR})
-
-target_include_directories(luci_interpreter_kernels SYSTEM PRIVATE
-        "${TensorFlowRuySource_DIR}"
-        "${TensorFlowGEMMLowpSource_DIR}"
-        "${TensorFlowEigenSource_DIR}"
-        "${TensorFlowSource_DIR}")
-target_link_libraries(luci_interpreter_kernels
-    PUBLIC luci_interpreter_core)
-
-if(NOT ENABLE_TEST)
-  return()
-endif(NOT ENABLE_TEST)
-
-nnas_find_package(GTest REQUIRED)
-
-set(TEST_SOURCES
-    Add.test.cpp
-    ArgMax.test.cpp
-    AveragePool2D.test.cpp
-    BatchToSpaceND.test.cpp
-    Concatenation.test.cpp
-    Conv2D.test.cpp
-    DepthToSpace.test.cpp
-    DepthwiseConv2D.test.cpp
-    Div.test.cpp
-    Elu.test.cpp
-    Exp.test.cpp
-    Floor.test.cpp
-    FloorDiv.test.cpp
-    Equal.test.cpp
-    FullyConnected.test.cpp
-    Greater.test.cpp
-    GreaterEqual.test.cpp
-    If.test.cpp
-    InstanceNorm.test.cpp
-    L2Normalize.test.cpp
-    L2Pool2D.test.cpp
-    LeakyRelu.test.cpp
-    Less.test.cpp
-    LessEqual.test.cpp
-    LocalResponseNormalization.test.cpp
-    LogicalAnd.test.cpp
-    LogicalNot.test.cpp
-    LogicalOr.test.cpp
-    Logistic.test.cpp
-    LogSoftmax.test.cpp
-    Maximum.test.cpp
-    MaxPool2D.test.cpp
-    Mean.test.cpp
-    Minimum.test.cpp
-    Mul.test.cpp
-    Neg.test.cpp
-    NotEqual.test.cpp
-    Pack.test.cpp
-    Pad.test.cpp
-    PadV2.test.cpp
-    Pow.test.cpp
-    Prelu.test.cpp
-    Relu.test.cpp
-    Relu6.test.cpp
-    Reshape.test.cpp
-    ResizeBilinear.test.cpp
-    ResizeNearestNeighbor.test.cpp
-    Reverse.test.cpp
-    Rsqrt.test.cpp
-    Slice.test.cpp
-    Softmax.test.cpp
-    SpaceToBatchND.test.cpp
-    SpaceToDepth.test.cpp
-    Split.test.cpp
-    StridedSlice.test.cpp
-    Sqrt.test.cpp
-    Square.test.cpp
-    SquaredDifference.test.cpp
-    Squeeze.test.cpp
-    Sub.test.cpp
-    Tanh.test.cpp
-    Transpose.test.cpp
-    TransposeConv.test.cpp
-    Unpack.test.cpp)
-
-list(APPEND TEST_SOURCES TestUtils.h TestUtils.cpp)
-
-GTest_AddTest(luci_interpreter_kernels_test ${TEST_SOURCES})
-target_link_libraries(luci_interpreter_kernels_test luci_interpreter_kernels)
+if (NOT LUCI_MICRO)
+    add_subdirectory(common)
+else()
+    add_subdirectory(micro)
+endif()
\ No newline at end of file
diff --git a/compiler/luci-interpreter/src/kernels/Concatenation.test.cpp b/compiler/luci-interpreter/src/kernels/Concatenation.test.cpp
deleted file mode 100644
index ee9b7d0d3..000000000
--- a/compiler/luci-interpreter/src/kernels/Concatenation.test.cpp
+++ /dev/null
@@ -1,196 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Concatenation.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(ConcatenationTest, Float)
-{
-  std::vector<float> input1_data{1, 2, 3, 4, 5, 6};
-  std::vector<float> input2_data{7, 8, 9, 10, 11, 12};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input2_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  ConcatenationParams params{};
-
-  // Try different 'axis' and expect different results.
-  {
-    params.axis = 0;
-    params.activation = luci::FusedActFunc::NONE;
-
-    Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-    kernel.configure();
-    kernel.execute();
-
-    EXPECT_THAT(extractTensorData<float>(output_tensor),
-                FloatArrayNear({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}));
-  }
-  {
-    params.axis = -2; // Same as '0'.
-    params.activation = luci::FusedActFunc::NONE;
-
-    Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-    kernel.configure();
-    kernel.execute();
-
-    EXPECT_THAT(extractTensorData<float>(output_tensor),
-                FloatArrayNear({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}));
-  }
-  {
-    params.axis = 1;
-    params.activation = luci::FusedActFunc::NONE;
-
-    Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-    kernel.configure();
-    kernel.execute();
-
-    EXPECT_THAT(extractTensorData<float>(output_tensor),
-                FloatArrayNear({1, 2, 3, 7, 8, 9, 4, 5, 6, 10, 11, 12}));
-  }
-  {
-    params.axis = -1; // Same as '1'.
-    params.activation = luci::FusedActFunc::NONE;
-
-    Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-    kernel.configure();
-    kernel.execute();
-
-    EXPECT_THAT(extractTensorData<float>(output_tensor),
-                FloatArrayNear({1, 2, 3, 7, 8, 9, 4, 5, 6, 10, 11, 12}));
-  }
-}
-
-TEST(ConcatenationTest, Input_Number_Check_NEG)
-{
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  ConcatenationParams params{};
-
-  params.axis = -1;
-  params.activation = luci::FusedActFunc::NONE;
-
-  Concatenation kernel({}, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(ConcatenationTest, Invalid_Axis_NEG)
-{
-  std::vector<float> input1_data{1, 2, 3, 4, 5, 6};
-  std::vector<float> input2_data{7, 8, 9, 10, 11, 12};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input2_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  ConcatenationParams params{};
-
-  params.axis = -3;
-  params.activation = luci::FusedActFunc::NONE;
-
-  Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(ConcatenationTest, Mismatching_Input_Type_NEG)
-{
-  std::vector<float> input1_data{1, 2, 3, 4, 5, 6};
-  std::vector<uint8_t> input2_data{7, 8, 9, 10, 11, 12};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::U8>({2, 3}, input2_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  ConcatenationParams params{};
-
-  params.axis = -1;
-  params.activation = luci::FusedActFunc::NONE;
-
-  Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(ConcatenationTest, Mismatching_Input_Dimension_Num_NEG)
-{
-  std::vector<float> input1_data{1, 2, 3, 4, 5, 6};
-  std::vector<float> input2_data{7, 8, 9, 10, 11, 12};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::FLOAT32>({1, 2, 3}, input2_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  ConcatenationParams params{};
-
-  params.axis = -1;
-  params.activation = luci::FusedActFunc::NONE;
-
-  Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(ConcatenationTest, Mismatching_Input_Dimension_NEG)
-{
-  std::vector<float> input1_data{1, 2, 3, 4, 5, 6};
-  std::vector<float> input2_data{7, 8, 9, 10, 11, 12, 13, 14, 15};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::FLOAT32>({3, 3}, input2_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  ConcatenationParams params{};
-
-  params.axis = -1;
-  params.activation = luci::FusedActFunc::NONE;
-
-  Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(ConcatenationTest, Unsupported_Configure_Type_NEG)
-{
-  std::vector<int8_t> input1_data{1, 2, 3, 4, 5, 6};
-  std::vector<int8_t> input2_data{7, 8, 9, 10, 11, 12};
-  Tensor input1_tensor = makeInputTensor<DataType::S8>({2, 3}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::S8>({2, 3}, input2_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S8);
-  ConcatenationParams params{};
-
-  params.axis = -1;
-  params.activation = luci::FusedActFunc::NONE;
-
-  Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-// TODO: Remove this test when concat w/ fused_activation is supported
-TEST(ConcatenationTest, With_Fused_Activation_NEG)
-{
-  std::vector<float> input1_data{1, 2, 3, 4, 5, 6};
-  std::vector<float> input2_data{7, 8, 9, 10, 11, 12};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, input2_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  ConcatenationParams params{};
-
-  params.axis = 1;
-  params.activation = luci::FusedActFunc::RELU;
-
-  Concatenation kernel({&input1_tensor, &input2_tensor}, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Conv2D.h b/compiler/luci-interpreter/src/kernels/Conv2D.h
deleted file mode 100644
index 86f73c251..000000000
--- a/compiler/luci-interpreter/src/kernels/Conv2D.h
+++ /dev/null
@@ -1,59 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_CONV2D_H
-#define LUCI_INTERPRETER_KERNELS_CONV2D_H
-
-#include "core/Kernel.h"
-#include "core/KernelParams.h"
-
-#include <memory>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Conv2D : public KernelWithParams<Conv2DParams>
-{
-public:
-  Conv2D(const Tensor *input, const Tensor *filter, const Tensor *bias, Tensor *output,
-         const Conv2DParams &params);
-
-  const Tensor *input() const { return _inputs[0]; }
-  const Tensor *filter() const { return _inputs[1]; }
-  const Tensor *bias() const { return _inputs[2]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  void evalFloat() const;
-  void evalQuantized() const;
-  void evalQuantizedPerChannel() const;
-  void evalQuantizedS16() const;
-
-private:
-  std::unique_ptr<Tensor> _im2col;
-  int32_t _padding_height{};
-  int32_t _padding_width{};
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_CONV2D_H
diff --git a/compiler/luci-interpreter/src/kernels/DepthToSpace.test.cpp b/compiler/luci-interpreter/src/kernels/DepthToSpace.test.cpp
deleted file mode 100644
index 3dee4ad36..000000000
--- a/compiler/luci-interpreter/src/kernels/DepthToSpace.test.cpp
+++ /dev/null
@@ -1,105 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/DepthToSpace.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-template <typename T> class DepthToSpaceTest : public ::testing::Test
-{
-};
-
-using DataTypes = ::testing::Types<float, uint8_t>;
-TYPED_TEST_CASE(DepthToSpaceTest, DataTypes);
-
-TYPED_TEST(DepthToSpaceTest, SimpleCase)
-{
-  std::vector<TypeParam> input_data{1, 2, 3, 4, 5, 6, 7, 8};
-  Shape input_shape{1, 1, 2, 4};
-  std::vector<TypeParam> output_data{1, 2, 5, 6, 3, 4, 7, 8};
-  std::vector<int32_t> output_shape{1, 2, 4, 1};
-
-  Tensor input_tensor = makeInputTensor<getElementType<TypeParam>()>(input_shape, input_data);
-  Tensor output_tensor = makeOutputTensor(getElementType<TypeParam>());
-
-  DepthToSpaceParams params{};
-  params.block_size = 2;
-
-  DepthToSpace kernel = DepthToSpace(&input_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<TypeParam>(output_tensor),
-              ::testing::ElementsAreArray(output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(output_shape));
-}
-
-TEST(DepthToSpaceTest, InvalidInputShape_NEG)
-{
-  std::vector<float> input_data{1, 2, 3, 4, 5, 6, 7, 8};
-  Shape input_shape{1, 2, 4};
-
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  DepthToSpaceParams params{};
-  params.block_size = 2;
-
-  DepthToSpace kernel = DepthToSpace(&input_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(DepthToSpaceTest, InOutTypeMismatch_NEG)
-{
-  std::vector<float> input_data{1, 2, 3, 4, 5, 6, 7, 8};
-  Shape input_shape{1, 1, 2, 4};
-
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::U8);
-
-  DepthToSpaceParams params{};
-  params.block_size = 2;
-
-  DepthToSpace kernel = DepthToSpace(&input_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(DepthToSpaceTest, InvalidBlockSize_NEG)
-{
-  std::vector<float> input_data{1, 2, 3, 4, 5, 6, 7, 8};
-  Shape input_shape{1, 1, 2, 4};
-
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  DepthToSpaceParams params{};
-  params.block_size = 3;
-
-  DepthToSpace kernel = DepthToSpace(&input_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/DepthwiseConv2D.test.cpp b/compiler/luci-interpreter/src/kernels/DepthwiseConv2D.test.cpp
deleted file mode 100644
index 3e2f434dd..000000000
--- a/compiler/luci-interpreter/src/kernels/DepthwiseConv2D.test.cpp
+++ /dev/null
@@ -1,478 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/DepthwiseConv2D.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(DepthwiseConv2DTest, Float)
-{
-  Shape input_shape{1, 4, 2, 2};
-  Shape filter_shape{1, 2, 2, 4};
-  Shape bias_shape{4};
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::FLOAT32>(filter_shape, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::FLOAT32>(bias_shape, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  std::vector<float> ref_output_data{
-    71,  0, 99,  0,  //
-    167, 0, 227, 28, //
-  };
-  EXPECT_THAT(extractTensorData<float>(output_tensor), FloatArrayNear(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 1, 4}));
-}
-
-TEST(DepthwiseConv2DTest, Uint8)
-{
-  std::vector<float> input_data{
-    1, 2, 7,  8,  // column 1
-    3, 4, 9,  10, // column 2
-    5, 6, 11, 12, // column 3
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-
-  std::pair<float, int32_t> input_quant_param = quantizationParams<uint8_t>(-63.5, 64);
-  std::pair<float, int32_t> output_quant_param = quantizationParams<uint8_t>(-127, 128);
-
-  Tensor input_tensor = makeInputTensor<DataType::U8>({1, 3, 2, 2}, input_quant_param.first,
-                                                      input_quant_param.second, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::U8>({1, 2, 2, 4}, input_quant_param.first,
-                                                       input_quant_param.second, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::S32>(
-    {4}, input_quant_param.first * input_quant_param.first, 0, bias_data);
-  Tensor output_tensor =
-    makeOutputTensor(DataType::U8, output_quant_param.first, output_quant_param.second);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 1;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::NONE;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  std::vector<float> ref_output_data{
-    71, -34, 99,  -20, //
-    91, -26, 127, -4,  //
-  };
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 1, 4}));
-}
-
-TEST(DepthwiseConv2DTest, SInt16)
-{
-  Shape input_shape{1, 4, 2, 2};
-  Shape filter_shape{1, 2, 2, 4};
-  Shape bias_shape{4};
-  std::vector<int32_t> ref_output_shape{1, 2, 1, 4};
-
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  std::vector<float> ref_output_data{
-    71,  0, 99,  0,  //
-    167, 0, 227, 28, //
-  };
-
-  Tensor input_tensor = makeInputTensor<DataType::S16>(input_shape, 0.25, 0, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::S16>(filter_shape, 0.2, 0, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::S64>(bias_shape, 0.25 * 0.2, 0, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.5, 0);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(ref_output_shape));
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-TEST(DepthwiseConv2DTest, SInt16_CWQ_weights)
-{
-  const int output_channels = 4;
-  Shape input_shape{1, 4, 2, 2};
-  Shape filter_shape{1, 2, 2, output_channels};
-  Shape bias_shape{4};
-  std::vector<int32_t> ref_output_shape{1, 2, 1, output_channels};
-
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  std::vector<float> ref_output_data{
-    71,  0, 99,  0,  //
-    167, 0, 227, 28, //
-  };
-
-  float input_scale = 0.25;
-  std::vector<float> filter_scales{0.2f, 1.f, 0.5f, 0.1f};
-  std::vector<float> bias_scales;
-  for (int i = 0; i < output_channels; ++i)
-    bias_scales.push_back(filter_scales[i] * input_scale);
-  std::vector<int32_t> zerop(4, 0);
-  Tensor input_tensor = makeInputTensor<DataType::S16>(input_shape, input_scale, 0, input_data);
-  Tensor filter_tensor =
-    makeInputTensor<DataType::S16>(filter_shape, filter_scales, zerop, 3, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::S64>(bias_shape, bias_scales, zerop, 0, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.5, 0);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(ref_output_shape));
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-TEST(DepthwiseConv2DTest, Uint8_CWQ_weights)
-{
-  const int output_channels = 4;
-  Shape input_shape{1, 3, 2, 2};
-  Shape filter_shape{1, 2, 2, output_channels};
-  Shape bias_shape{4};
-  std::vector<int32_t> ref_output_shape{1, 2, 1, output_channels};
-
-  std::vector<float> input_data{
-    1, 2, 7,  8,  //
-    3, 4, 9,  10, //
-    5, 6, 11, 12, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  std::vector<float> ref_output_data{
-    71, -34, 99,  -20, //
-    91, -26, 127, -4,  //
-  };
-
-  std::pair<float, int32_t> input_quant_param = quantizationParams<uint8_t>(0, 16);
-  std::pair<float, int32_t> output_quant_param = quantizationParams<uint8_t>(-127, 128);
-
-  std::vector<std::pair<float, int32_t>> filter_quant_params;
-  filter_quant_params.push_back(quantizationParams<uint8_t>(-9, 13));
-  filter_quant_params.push_back(quantizationParams<uint8_t>(-14, 10));
-  filter_quant_params.push_back(quantizationParams<uint8_t>(-11, 15));
-  filter_quant_params.push_back(quantizationParams<uint8_t>(-16, 12));
-
-  std::vector<float> filter_scales;
-  std::vector<int32_t> filter_zerops;
-  for (auto iter : filter_quant_params)
-  {
-    filter_scales.push_back(iter.first);
-    filter_zerops.push_back(iter.second);
-  }
-
-  std::vector<float> bias_scales;
-  for (int i = 0; i < output_channels; ++i)
-    bias_scales.push_back(filter_quant_params[i].first * input_quant_param.first);
-  std::vector<int32_t> zerop(output_channels, 0);
-
-  Tensor input_tensor = makeInputTensor<DataType::U8>(input_shape, input_quant_param.first,
-                                                      input_quant_param.second, input_data);
-  Tensor filter_tensor =
-    makeInputTensor<DataType::U8>(filter_shape, filter_scales, filter_zerops, 3, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::S32>(bias_shape, bias_scales, zerop, 0, bias_data);
-  Tensor output_tensor =
-    makeOutputTensor(DataType::U8, output_quant_param.first, output_quant_param.second);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 1;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::NONE;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(ref_output_shape));
-  EXPECT_THAT(dequantizeTensorData(output_tensor),
-              FloatArrayNear(ref_output_data, output_quant_param.first));
-}
-
-TEST(DepthwiseConv2DTest, InvalidBiasType_NEG)
-{
-  Shape input_shape{1, 4, 2, 2};
-  Shape filter_shape{1, 2, 2, 4};
-  Shape bias_shape{4};
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<int32_t> bias_data{1, 2, 3, 4};
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::FLOAT32>(filter_shape, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::S32>(bias_shape, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(DepthwiseConv2DTest, InOutTypeMismatch_NEG)
-{
-  Shape input_shape{1, 4, 2, 2};
-  Shape filter_shape{1, 2, 2, 4};
-  Shape bias_shape{4};
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::FLOAT32>(filter_shape, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::FLOAT32>(bias_shape, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::U8);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(DepthwiseConv2DTest, InvalidInputShape_NEG)
-{
-  Shape input_shape{4, 2, 2};
-  Shape filter_shape{2, 2, 4};
-  Shape bias_shape{4};
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::FLOAT32>(filter_shape, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::FLOAT32>(bias_shape, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(DepthwiseConv2DTest, InvalidFilterShape_NEG)
-{
-  Shape input_shape{1, 4, 2, 2};
-  Shape filter_shape{2, 1, 2, 4};
-  Shape bias_shape{4};
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::FLOAT32>(filter_shape, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::FLOAT32>(bias_shape, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(DepthwiseConv2DTest, InvalidBiasDim_NEG)
-{
-  Shape input_shape{1, 4, 2, 2};
-  Shape filter_shape{1, 2, 4, 2};
-  Shape bias_shape{4};
-  std::vector<float> input_data{
-    1,  2,  7,  8,  //
-    3,  4,  9,  10, //
-    5,  6,  11, 12, //
-    13, 14, 15, 16, //
-  };
-  std::vector<float> filter_data{
-    1,  2,   3,   4,   //
-    -9, 10,  -11, 12,  //
-    5,  6,   7,   8,   //
-    13, -14, 15,  -16, //
-  };
-  std::vector<float> bias_data{1, 2, 3, 4};
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor filter_tensor = makeInputTensor<DataType::FLOAT32>(filter_shape, filter_data);
-  Tensor bias_tensor = makeInputTensor<DataType::FLOAT32>(bias_shape, bias_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  DepthwiseConv2DParams params{};
-  params.padding = Padding::VALID;
-  params.depth_multiplier = 2;
-  params.stride_height = 2;
-  params.stride_width = 1;
-  params.dilation_height_factor = 1;
-  params.dilation_width_factor = 1;
-  params.activation = Activation::RELU;
-
-  DepthwiseConv2D kernel(&input_tensor, &filter_tensor, &bias_tensor, &output_tensor, params);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Equal.test.cpp b/compiler/luci-interpreter/src/kernels/Equal.test.cpp
deleted file mode 100644
index ba2827ba9..000000000
--- a/compiler/luci-interpreter/src/kernels/Equal.test.cpp
+++ /dev/null
@@ -1,187 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Equal.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(EqualTest, FloatSimple)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-    -1,  0,   1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    false, true, false, // Row 1
-    false, true, false, // Row 2
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Equal kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({2, 3}));
-}
-
-TEST(EqualTest, FloatBroardcast)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-    -1,  0,   1,   // Row 3
-    0.9, 0.7, 0.5, // Row 4
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    false, true,  false, // Row 1
-    false, false, false, // Row 2
-    false, false, false, // Row 3
-    true,  true,  true,  // Row 4
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({4, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Equal kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({4, 3}));
-}
-
-// Choose min / max in such a way that there are exactly 256 units to avoid rounding errors.
-const float F_MIN = -128.0 / 128.0;
-const float F_MAX = 127.0 / 128.0;
-
-TEST(EqualTest, Uint8Quantized)
-{
-  std::vector<float> x_data{
-    0.5, 0.5, 0.7,  0.9, // Row 1
-    1,   0,   0.05, -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.5, 0.55, 0.5, // Row 1
-    -1,  0,   0.05, 1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    false, true, false, false, // Row 1
-    false, true, true,  false, // Row 2
-  };
-
-  std::pair<float, int32_t> x_quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, x_quant_param.first, x_quant_param.second, x_data);
-
-  std::pair<float, int32_t> y_quant_param = quantizationParams<uint8_t>(F_MIN * 2, F_MAX * 2);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, y_quant_param.first, y_quant_param.second, y_data);
-
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Equal kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(EqualTest, Uint8QuantizedBroadcast)
-{
-  std::vector<float> x_data{
-    0.4,  -0.8, 0.7,  0.3, // Row 1
-    -0.5, 0.1,  0,    0.5, // Row 2
-    1,    0,    0.05, -1,  // Row 3
-    -1,   0.05, 0,    1,   // Row 4
-  };
-
-  std::vector<float> y_data{
-    -1, 0.05, 0, 1, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    false, false, false, false, // Row 1
-    false, false, true,  false, // Row 2
-    false, false, false, false, // Row 3
-    true,  true,  true,  true,  // Row 4
-  };
-
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 4, 4, 1}, quant_param.first, quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 1, 4, 1}, quant_param.first, quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Equal kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 4, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(EqualTest, Input_Type_Mismatch_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::U8>({1}, {1});
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Equal kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(EqualTest, Input_Output_Type_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  Equal kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Floor.test.cpp b/compiler/luci-interpreter/src/kernels/Floor.test.cpp
deleted file mode 100644
index d90d611d9..000000000
--- a/compiler/luci-interpreter/src/kernels/Floor.test.cpp
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Floor.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(FloorTest, SimpleFloat)
-{
-  std::initializer_list<int32_t> input_shape{1, 2, 4, 1};
-  std::vector<float> input_data{
-    0.2, 8.6, 2.4,  4.3,  // Row 1
-    3,   7.1, 10.5, -0.9, // Row 2
-  };
-
-  std::initializer_list<int32_t> ref_output_shape{1, 2, 4, 1};
-  std::vector<float> ref_output_data{
-    0, 8, 2,  4,  // Row 1
-    3, 7, 10, -1, // Row 2
-  };
-
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  Floor kernel(&input_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<float>(output_tensor), FloatArrayNear(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(ref_output_shape));
-}
-
-TEST(FloorTest, Input_Output_Type_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor output_tensor = makeOutputTensor(DataType::S32);
-
-  Floor kernel(&input_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Greater.cpp b/compiler/luci-interpreter/src/kernels/Greater.cpp
deleted file mode 100644
index f0dd2db36..000000000
--- a/compiler/luci-interpreter/src/kernels/Greater.cpp
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Greater.h"
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/comparisons.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-Greater::Greater(const Tensor *x, const Tensor *y, Tensor *output) : Kernel({x, y}, {output}) {}
-
-void Greater::configure()
-{
-  LUCI_INTERPRETER_CHECK(x()->element_type() == y()->element_type());
-  LUCI_INTERPRETER_CHECK(output()->element_type() == DataType::BOOL);
-
-  if (x()->element_type() == DataType::U8)
-  {
-    quantizeMultiplierSmallerThanOneExp(x()->scale(), &_x_multiplier, &_x_shift);
-    quantizeMultiplierSmallerThanOneExp(y()->scale(), &_y_multiplier, &_y_shift);
-  }
-  output()->resize(calculateShapeForBroadcast(x()->shape(), y()->shape()));
-}
-
-void Greater::execute() const
-{
-  switch (x()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::U8:
-      evalQuantized();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void Greater::evalFloat() const
-{
-  const auto x_data = getTensorData<float>(x());
-  const auto y_data = getTensorData<float>(y());
-  auto output_data = getTensorData<bool>(output());
-
-  tflite::ComparisonParams op_params;
-  op_params.is_broadcast = x()->shape() != y()->shape();
-
-  if (op_params.is_broadcast)
-  {
-    tflite::reference_ops::Broadcast4DSlowGreater(op_params, getTensorShape(x()), x_data,
-                                                  getTensorShape(y()), y_data,
-                                                  getTensorShape(output()), output_data);
-  }
-  else
-  {
-    tflite::reference_ops::Greater(op_params, getTensorShape(x()), x_data, getTensorShape(y()),
-                                   y_data, getTensorShape(output()), output_data);
-  }
-}
-
-void Greater::evalQuantized() const
-{
-  const auto x_data = getTensorData<uint8_t>(x());
-  const auto y_data = getTensorData<uint8_t>(y());
-  auto output_data = getTensorData<bool>(output());
-
-  tflite::ComparisonParams op_params;
-  op_params.left_shift = 8;
-  op_params.input1_offset = -x()->zero_point(); // Note the '-'
-  op_params.input1_shift = _x_shift;
-  op_params.input1_multiplier = _x_multiplier;
-  op_params.input2_offset = -y()->zero_point(); // Note the '-'
-  op_params.input2_shift = _y_shift;
-  op_params.input2_multiplier = _y_multiplier;
-  op_params.is_broadcast = x()->shape() != y()->shape();
-
-  if (op_params.is_broadcast)
-  {
-    tflite::reference_ops::Broadcast4DSlowGreaterWithScaling(op_params, getTensorShape(x()), x_data,
-                                                             getTensorShape(y()), y_data,
-                                                             getTensorShape(output()), output_data);
-  }
-  else
-  {
-    tflite::reference_ops::GreaterWithScaling(op_params, getTensorShape(x()), x_data,
-                                              getTensorShape(y()), y_data, getTensorShape(output()),
-                                              output_data);
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Greater.h b/compiler/luci-interpreter/src/kernels/Greater.h
deleted file mode 100644
index a65d29f5c..000000000
--- a/compiler/luci-interpreter/src/kernels/Greater.h
+++ /dev/null
@@ -1,53 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_GREATER_H
-#define LUCI_INTERPRETER_KERNELS_GREATER_H
-
-#include "core/Kernel.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Greater : public Kernel
-{
-public:
-  Greater(const Tensor *x, const Tensor *y, Tensor *output);
-
-  const Tensor *x() const { return _inputs[0]; }
-  const Tensor *y() const { return _inputs[1]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  void evalFloat() const;
-  void evalQuantized() const;
-
-private:
-  int32_t _x_multiplier = 0;
-  int32_t _x_shift = 0;
-  int32_t _y_multiplier = 0;
-  int32_t _y_shift = 0;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_GREATER_H
diff --git a/compiler/luci-interpreter/src/kernels/Greater.test.cpp b/compiler/luci-interpreter/src/kernels/Greater.test.cpp
deleted file mode 100644
index 3fcc86603..000000000
--- a/compiler/luci-interpreter/src/kernels/Greater.test.cpp
+++ /dev/null
@@ -1,214 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Greater.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(GreaterTest, FloatSimple)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-    -1,  0,   1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    false, false, true,  // Row 1
-    true,  false, false, // Row 2
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Greater kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({2, 3}));
-}
-
-TEST(GreaterTest, FloatBroardcast)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-    -1,  0,   1,   // Row 3
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    false, false, true,  // Row 1
-    true,  false, false, // Row 2
-    false, false, true,  // Row 3
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({3, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Greater kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({3, 3}));
-}
-
-// Choose min / max in such a way that there are exactly 256 units to avoid rounding errors.
-const float F_MIN = -128.0 / 128.0;
-const float F_MAX = 127.0 / 128.0;
-
-TEST(GreaterTest, Uint8Quantized)
-{
-  std::vector<float> x_data{
-    0.5, 0.6, 0.7,  0.9, // Row 1
-    1,   0,   0.05, -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.6,  0.6, 0.5, // Row 1
-    -1,  0.05, 0,   1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    false, false, true, true,  // Row 1
-    true,  false, true, false, // Row 2
-  };
-
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, quant_param.first, quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, quant_param.first, quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Greater kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(GreaterTest, Uint8QuantizedRescale)
-{
-  std::vector<float> x_data{
-    0.5, 0.6, 0.7,  0.9, // Row 1
-    1,   0,   0.05, -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.6,  0.6, 0.5, // Row 1
-    -1,  0.05, 0,   1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    false, false, true, true,  // Row 1
-    true,  false, true, false, // Row 2
-  };
-
-  std::pair<float, int32_t> x_quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  std::pair<float, int32_t> y_quant_param = quantizationParams<uint8_t>(F_MIN * 2, F_MAX * 3);
-
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, x_quant_param.first, x_quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, y_quant_param.first, y_quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Greater kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(GreaterTest, Uint8QuantizedBroadcast)
-{
-  std::vector<float> x_data{
-    0.4,  -0.8, 0.7,  0.3, // Row 1
-    -0.5, 0.1,  0,    0.5, // Row 2
-    1,    0,    0.05, -1,  // Row 3
-  };
-
-  std::vector<float> y_data{
-    -1, 0.05, 0, 1, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    true, false, true,  false, // Row 1
-    true, true,  false, false, // Row 2
-    true, false, true,  false, // Row 3
-  };
-
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 3, 4, 1}, quant_param.first, quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 1, 4, 1}, quant_param.first, quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Greater kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 3, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(GreaterTest, Input_Type_Mismatch_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::U8>({1}, {1});
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Greater kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(GreaterTest, Input_Output_Type_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  Greater kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/If.test.cpp b/compiler/luci-interpreter/src/kernels/If.test.cpp
deleted file mode 100644
index 0dba310d9..000000000
--- a/compiler/luci-interpreter/src/kernels/If.test.cpp
+++ /dev/null
@@ -1,141 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2019 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "core/RuntimeModule.h"
-#include "kernels/Add.h"
-#include "kernels/If.h"
-#include "kernels/Mul.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-RuntimeGraph *buildAddSubgraph(RuntimeModule *module)
-{
-  RuntimeGraph *graph = module->addGraph();
-  Tensor *input1 = graph->addTensor(
-    std::make_unique<Tensor>(DataType::FLOAT32, Shape{}, AffineQuantization{}, ""));
-  Tensor *input2 = graph->addTensor(
-    std::make_unique<Tensor>(DataType::FLOAT32, Shape{}, AffineQuantization{}, ""));
-  Tensor *output = graph->addTensor(
-    std::make_unique<Tensor>(DataType::FLOAT32, Shape{}, AffineQuantization{}, ""));
-
-  graph->setInputTensors({input1, input2});
-  graph->setOutputTensors({output});
-
-  AddParams params{};
-  params.activation = Activation::NONE;
-  graph->addKernel(std::make_unique<Add>(input1, input2, output, params));
-
-  return graph;
-}
-
-RuntimeGraph *buildMulSubgraph(RuntimeModule *module)
-{
-  RuntimeGraph *graph = module->addGraph();
-  Tensor *input1 = graph->addTensor(
-    std::make_unique<Tensor>(DataType::FLOAT32, Shape{}, AffineQuantization{}, ""));
-  Tensor *input2 = graph->addTensor(
-    std::make_unique<Tensor>(DataType::FLOAT32, Shape{}, AffineQuantization{}, ""));
-  Tensor *output = graph->addTensor(
-    std::make_unique<Tensor>(DataType::FLOAT32, Shape{}, AffineQuantization{}, ""));
-
-  graph->setInputTensors({input1, input2});
-  graph->setOutputTensors({output});
-
-  MulParams params{};
-  params.activation = Activation::NONE;
-  graph->addKernel(std::make_unique<Mul>(input1, input2, output, params));
-
-  return graph;
-}
-
-TEST(IfTest, CondTrue)
-{
-  Tensor cond = makeInputTensor<DataType::BOOL>({1}, {true});
-  Tensor input1 = makeInputTensor<DataType::FLOAT32>({2}, {5, 7});
-  Tensor input2 = makeInputTensor<DataType::FLOAT32>({1, 2}, {1, 2});
-  Tensor output = makeOutputTensor(DataType::FLOAT32);
-
-  RuntimeModule module(nullptr);
-  RuntimeGraph *then_graph = buildAddSubgraph(&module);
-  RuntimeGraph *else_graph = buildMulSubgraph(&module);
-
-  If kernel(&cond, {&input1, &input2}, {&output}, then_graph, else_graph);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<float>(output), FloatArrayNear({6, 9}));
-}
-
-TEST(IfTest, CondFalse)
-{
-  Tensor cond = makeInputTensor<DataType::BOOL>({1}, {false});
-  Tensor input1 = makeInputTensor<DataType::FLOAT32>({2}, {5, 7});
-  Tensor input2 = makeInputTensor<DataType::FLOAT32>({1, 2}, {1, 2});
-  Tensor output = makeOutputTensor(DataType::FLOAT32);
-
-  RuntimeModule module(nullptr);
-  RuntimeGraph *then_graph = buildAddSubgraph(&module);
-  RuntimeGraph *else_graph = buildMulSubgraph(&module);
-
-  If kernel(&cond, {&input1, &input2}, {&output}, then_graph, else_graph);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<float>(output), FloatArrayNear({5, 14}));
-}
-
-TEST(IfTest, InvalidCondType_NEG)
-{
-  Tensor cond = makeInputTensor<DataType::FLOAT32>({1}, {1});
-  Tensor input1 = makeInputTensor<DataType::FLOAT32>({2}, {5, 7});
-  Tensor input2 = makeInputTensor<DataType::FLOAT32>({1, 2}, {1, 2});
-  Tensor output = makeOutputTensor(DataType::FLOAT32);
-
-  RuntimeModule module(nullptr);
-  RuntimeGraph *then_graph = buildAddSubgraph(&module);
-  RuntimeGraph *else_graph = buildMulSubgraph(&module);
-
-  If kernel(&cond, {&input1, &input2}, {&output}, then_graph, else_graph);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(IfTest, InvalidCondElementNum_NEG)
-{
-  Tensor cond = makeInputTensor<DataType::BOOL>({2}, {false, true});
-  Tensor input1 = makeInputTensor<DataType::FLOAT32>({2}, {5, 7});
-  Tensor input2 = makeInputTensor<DataType::FLOAT32>({1, 2}, {1, 2});
-  Tensor output = makeOutputTensor(DataType::FLOAT32);
-
-  RuntimeModule module(nullptr);
-  RuntimeGraph *then_graph = buildAddSubgraph(&module);
-  RuntimeGraph *else_graph = buildMulSubgraph(&module);
-
-  If kernel(&cond, {&input1, &input2}, {&output}, then_graph, else_graph);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/InstanceNorm.cpp b/compiler/luci-interpreter/src/kernels/InstanceNorm.cpp
deleted file mode 100644
index 22a329be6..000000000
--- a/compiler/luci-interpreter/src/kernels/InstanceNorm.cpp
+++ /dev/null
@@ -1,121 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/InstanceNorm.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/common.h>
-#include <cmath>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-InstanceNorm::InstanceNorm(const Tensor *input, const Tensor *gamma, const Tensor *beta,
-                           Tensor *output, const InstanceNormParams &params)
-  : KernelWithParams<InstanceNormParams>({input, gamma, beta}, {output}, params)
-{
-}
-
-void InstanceNorm::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->shape().num_dims() == 4);
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-  LUCI_INTERPRETER_CHECK(gamma()->element_type() == input()->element_type());
-  LUCI_INTERPRETER_CHECK(gamma()->shape().num_dims() == 1);
-  LUCI_INTERPRETER_CHECK(gamma()->shape().dim(0) == input()->shape().dim(3) ||
-                         gamma()->shape().dim(0) == 1);
-  LUCI_INTERPRETER_CHECK(beta()->element_type() == input()->element_type());
-  LUCI_INTERPRETER_CHECK(beta()->shape().num_dims() == 1);
-  LUCI_INTERPRETER_CHECK(beta()->shape().dim(0) == input()->shape().dim(3) ||
-                         beta()->shape().dim(0) == 1);
-  output()->resize(input()->shape());
-}
-
-void InstanceNorm::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void InstanceNorm::evalFloat() const
-{
-  float activation_min, activation_max;
-  calculateActivationRange(params().activation, &activation_min, &activation_max);
-  auto input_shape = getTensorShape(input());
-  auto output_shape = getTensorShape(output());
-  const int32_t batches = tflite::MatchingDim(input_shape, 0, output_shape, 0);
-  const int32_t heights = tflite::MatchingDim(input_shape, 1, output_shape, 1);
-  const int32_t widths = tflite::MatchingDim(input_shape, 2, output_shape, 2);
-  const int32_t channels = tflite::MatchingDim(input_shape, 3, output_shape, 3);
-  const float *input_data = getTensorData<float>(input());
-  const float *gamma_data = getTensorData<float>(gamma());
-  auto gamma_shape = getTensorShape(gamma());
-  bool single_gamma = gamma_shape.DimensionsCount() == 1 && gamma_shape.Dims(0) == 1;
-  const float *beta_data = getTensorData<float>(beta());
-  auto beta_shape = getTensorShape(beta());
-  bool single_beta = beta_shape.DimensionsCount() == 1 && beta_shape.Dims(0) == 1;
-  float *output_data = getTensorData<float>(output());
-  for (int32_t batch = 0; batch < batches; batch++)
-  {
-    for (int32_t channel = 0; channel < channels; channel++)
-    {
-      double sum = 0.0f;
-      double square_sum = 0.0f;
-      int32_t size = heights * widths;
-      for (int32_t height = 0; height < heights; height++)
-      {
-        for (int32_t width = 0; width < widths; width++)
-        {
-          double input_val = input_data[tflite::Offset(input_shape, batch, height, width, channel)];
-          sum += input_val;
-          square_sum += (input_val * input_val);
-        }
-      }
-      double mean = sum / size;
-      double var = square_sum / size - mean * mean;
-
-      double gamma = single_gamma ? gamma_data[0] : gamma_data[channel];
-      double beta = single_beta ? beta_data[0] : beta_data[channel];
-      double a = gamma / (std::sqrt(var + params().epsilon));
-      double b = -mean * a + beta;
-
-      for (int32_t height = 0; height < heights; height++)
-      {
-        for (int32_t width = 0; width < widths; width++)
-        {
-          double input_value =
-            input_data[tflite::Offset(output_shape, batch, height, width, channel)];
-          double output_value = input_value * a + b;
-          output_data[tflite::Offset(output_shape, batch, height, width, channel)] =
-            tflite::ActivationFunctionWithMinMax((float)output_value, activation_min,
-                                                 activation_max);
-        }
-      }
-    }
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/LeakyRelu.cpp b/compiler/luci-interpreter/src/kernels/LeakyRelu.cpp
deleted file mode 100644
index f468da5d3..000000000
--- a/compiler/luci-interpreter/src/kernels/LeakyRelu.cpp
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/LeakyRelu.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/reference_ops.h>
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-LeakyRelu::LeakyRelu(const Tensor *input, Tensor *output, const LeakyReluParams &params)
-  : KernelWithParams<LeakyReluParams>({input}, {output}, params)
-{
-}
-
-void LeakyRelu::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-  if (input()->element_type() == DataType::U8)
-  {
-    double alpha_multiplier = input()->scale() * params().alpha / output()->scale();
-    quantizeMultiplier(alpha_multiplier, &_output_multiplier_alpha, &_output_shift_alpha);
-    double identity_multiplier = input()->scale() / output()->scale();
-    quantizeMultiplier(identity_multiplier, &_output_multiplier_identity, &_output_shift_identity);
-  }
-  output()->resize(input()->shape());
-}
-
-void LeakyRelu::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::U8:
-      evalQuantized();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void LeakyRelu::evalFloat() const
-{
-  tflite::LeakyReluParams op_params{};
-  op_params.alpha = params().alpha;
-  tflite::optimized_ops::LeakyRelu(op_params, getTensorShape(input()),
-                                   getTensorData<float>(input()), getTensorShape(output()),
-                                   getTensorData<float>(output()));
-}
-
-void LeakyRelu::evalQuantized() const
-{
-  tflite::LeakyReluParams op_params{};
-  op_params.input_offset = input()->zero_point();
-  op_params.output_offset = output()->zero_point();
-  op_params.output_multiplier_alpha = _output_multiplier_alpha;
-  op_params.output_shift_alpha = _output_shift_alpha;
-  op_params.output_multiplier_identity = _output_multiplier_identity;
-  op_params.output_shift_identity = _output_shift_identity;
-
-  tflite::reference_ops::QuantizeLeakyRelu(
-    op_params, getTensorShape(input()), getTensorData<uint8_t>(input()), getTensorShape(output()),
-    getTensorData<uint8_t>(output()));
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Less.cpp b/compiler/luci-interpreter/src/kernels/Less.cpp
deleted file mode 100644
index 041444926..000000000
--- a/compiler/luci-interpreter/src/kernels/Less.cpp
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Less.h"
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/comparisons.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-Less::Less(const Tensor *x, const Tensor *y, Tensor *output) : Kernel({x, y}, {output}) {}
-
-void Less::configure()
-{
-  LUCI_INTERPRETER_CHECK(x()->element_type() == y()->element_type());
-  LUCI_INTERPRETER_CHECK(output()->element_type() == DataType::BOOL);
-
-  if (x()->element_type() == DataType::U8)
-  {
-    quantizeMultiplierSmallerThanOneExp(x()->scale(), &_x_multiplier, &_x_shift);
-    quantizeMultiplierSmallerThanOneExp(y()->scale(), &_y_multiplier, &_y_shift);
-  }
-  output()->resize(calculateShapeForBroadcast(x()->shape(), y()->shape()));
-}
-
-void Less::execute() const
-{
-  switch (x()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::U8:
-      evalQuantized();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void Less::evalFloat() const
-{
-  const auto x_data = getTensorData<float>(x());
-  const auto y_data = getTensorData<float>(y());
-  auto output_data = getTensorData<bool>(output());
-
-  tflite::ComparisonParams op_params;
-  op_params.is_broadcast = x()->shape() != y()->shape();
-
-  if (op_params.is_broadcast)
-  {
-    tflite::reference_ops::Broadcast4DSlowLess(op_params, getTensorShape(x()), x_data,
-                                               getTensorShape(y()), y_data,
-                                               getTensorShape(output()), output_data);
-  }
-  else
-  {
-    tflite::reference_ops::Less(op_params, getTensorShape(x()), x_data, getTensorShape(y()), y_data,
-                                getTensorShape(output()), output_data);
-  }
-}
-
-void Less::evalQuantized() const
-{
-  const auto x_data = getTensorData<uint8_t>(x());
-  const auto y_data = getTensorData<uint8_t>(y());
-  auto output_data = getTensorData<bool>(output());
-
-  tflite::ComparisonParams op_params;
-  op_params.left_shift = 8;
-  op_params.input1_offset = -x()->zero_point(); // Note the '-'
-  op_params.input1_shift = _x_shift;
-  op_params.input1_multiplier = _x_multiplier;
-  op_params.input2_offset = -y()->zero_point(); // Note the '-'
-  op_params.input2_shift = _y_shift;
-  op_params.input2_multiplier = _y_multiplier;
-  op_params.is_broadcast = x()->shape() != y()->shape();
-
-  if (op_params.is_broadcast)
-  {
-    tflite::reference_ops::Broadcast4DSlowLessWithScaling(op_params, getTensorShape(x()), x_data,
-                                                          getTensorShape(y()), y_data,
-                                                          getTensorShape(output()), output_data);
-  }
-  else
-  {
-    tflite::reference_ops::LessWithScaling(op_params, getTensorShape(x()), x_data,
-                                           getTensorShape(y()), y_data, getTensorShape(output()),
-                                           output_data);
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Less.test.cpp b/compiler/luci-interpreter/src/kernels/Less.test.cpp
deleted file mode 100644
index 2972bd559..000000000
--- a/compiler/luci-interpreter/src/kernels/Less.test.cpp
+++ /dev/null
@@ -1,214 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Less.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(LessTest, FloatSimple)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-    -1,  0,   1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    true,  false, false, // Row 1
-    false, false, true,  // Row 2
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Less kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({2, 3}));
-}
-
-TEST(LessTest, FloatBroardcast)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-    -1,  0,   1,   // Row 3
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    true,  false, false, // Row 1
-    false, true,  true,  // Row 2
-    true,  true,  false, // Row 3
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({3, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Less kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({3, 3}));
-}
-
-// Choose min / max in such a way that there are exactly 256 units to avoid rounding errors.
-const float F_MIN = -128.0 / 128.0;
-const float F_MAX = 127.0 / 128.0;
-
-TEST(LessTest, Uint8Quantized)
-{
-  std::vector<float> x_data{
-    0.5, 0.6, 0.7,  0.9, // Row 1
-    1,   0,   0.05, -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.6,  0.55, 0.5, // Row 1
-    -1,  0.05, 0,    1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    true,  false, false, false, // Row 1
-    false, true,  false, true,  // Row 2
-  };
-
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, quant_param.first, quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, quant_param.first, quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Less kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(LessTest, Uint8QuantizedRescale)
-{
-  std::vector<float> x_data{
-    0.5, 0.6, 0.7,  0.9, // Row 1
-    1,   0,   0.05, -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.6,  0.6, 0.5, // Row 1
-    -1,  0.05, 0,   1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    true,  false, false, false, // Row 1
-    false, true,  false, true,  // Row 2
-  };
-
-  std::pair<float, int32_t> x_quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  std::pair<float, int32_t> y_quant_param = quantizationParams<uint8_t>(F_MIN * 1.2, F_MAX * 1.5);
-
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, x_quant_param.first, x_quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, y_quant_param.first, y_quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Less kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(LessTest, Uint8QuantizedBroadcast)
-{
-  std::vector<float> x_data{
-    0.4,  -0.8, 0.7,  0.3, // Row 1
-    -0.5, 0.1,  0,    0.5, // Row 2
-    1,    0,    0.05, -1,  // Row 3
-  };
-
-  std::vector<float> y_data{
-    -1, 0.05, 0, 1, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    false, true,  false, true, // Row 1
-    false, false, false, true, // Row 2
-    false, true,  false, true, // Row 3
-  };
-
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 3, 4, 1}, quant_param.first, quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 1, 4, 1}, quant_param.first, quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Less kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 3, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(LessTest, Input_Type_Mismatch_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::U8>({1}, {1});
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  Less kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(LessTest, Input_Output_Type_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  Less kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/LocalResponseNormalization.cpp b/compiler/luci-interpreter/src/kernels/LocalResponseNormalization.cpp
deleted file mode 100644
index fd2ec41a1..000000000
--- a/compiler/luci-interpreter/src/kernels/LocalResponseNormalization.cpp
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/LocalResponseNormalization.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-LocalResponseNormalization::LocalResponseNormalization(
-  const Tensor *input, Tensor *output, const LocalResponseNormalizationParams &params)
-  : KernelWithParams<LocalResponseNormalizationParams>({input}, {output}, params)
-{
-}
-
-void LocalResponseNormalization::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->shape().num_dims() == 4);
-  LUCI_INTERPRETER_CHECK(output()->element_type() == DataType::FLOAT32);
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-  output()->resize(input()->shape());
-}
-
-void LocalResponseNormalization::execute() const
-{
-  switch (output()->element_type())
-  {
-    case DataType::FLOAT32:
-      tflite::LocalResponseNormalizationParams op_params;
-      op_params.range = params().radius;
-      op_params.bias = params().bias;
-      op_params.alpha = params().alpha;
-      op_params.beta = params().beta;
-      tflite::optimized_ops::LocalResponseNormalization(
-        op_params, getTensorShape(input()), getTensorData<float>(input()), getTensorShape(output()),
-        getTensorData<float>(output()));
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/LogSoftmax.h b/compiler/luci-interpreter/src/kernels/LogSoftmax.h
deleted file mode 100644
index 18477fbe3..000000000
--- a/compiler/luci-interpreter/src/kernels/LogSoftmax.h
+++ /dev/null
@@ -1,48 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_LOGSOFTMAX_H
-#define LUCI_INTERPRETER_KERNELS_LOGSOFTMAX_H
-
-#include "core/Kernel.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class LogSoftmax : public Kernel
-{
-public:
-  LogSoftmax(const Tensor *input, Tensor *output);
-
-  const Tensor *input() const { return _inputs[0]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  void evalFloat() const;
-  void evalQuantized() const;
-
-  float _table[256];
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_LOGSOFTMAX_H
diff --git a/compiler/luci-interpreter/src/kernels/LogicalNot.cpp b/compiler/luci-interpreter/src/kernels/LogicalNot.cpp
deleted file mode 100644
index 65ab961aa..000000000
--- a/compiler/luci-interpreter/src/kernels/LogicalNot.cpp
+++ /dev/null
@@ -1,60 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/LogicalNot.h"
-
-#include "kernels/Utils.h"
-
-#include "kernels/BinaryOpCommon.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-LogicalNot::LogicalNot(const Tensor *input, Tensor *output) : Kernel({input}, {output}) {}
-
-void LogicalNot::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-  output()->resize(input()->shape());
-}
-
-void LogicalNot::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::BOOL:
-      evalLogicalNot();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-inline void LogicalNot::evalLogicalNot() const
-{
-  const int size = tflite::MatchingFlatSize(getTensorShape(input()), getTensorShape(output()));
-  bool *output_data = getTensorData<bool>(output());
-  const bool *input_data = getTensorData<bool>(input());
-  for (int i = 0; i < size; ++i)
-  {
-    output_data[i] = !input_data[i];
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/LogicalNot.test.cpp b/compiler/luci-interpreter/src/kernels/LogicalNot.test.cpp
deleted file mode 100644
index dccb81102..000000000
--- a/compiler/luci-interpreter/src/kernels/LogicalNot.test.cpp
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/LogicalNot.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(LogicalNotTest, Basic)
-{
-  Shape input_shape{1, 1, 1, 4};
-  Tensor input_tensor = makeInputTensor<DataType::BOOL>(input_shape, {true, false, false, true});
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  LogicalNot kernel(&input_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor),
-              ::testing::ElementsAre(false, true, true, false));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAre(1, 1, 1, 4));
-}
-
-TEST(LogicalNotTest, OutputTypeInvalid_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::BOOL>({1, 1, 1, 4}, {true, false, false, true});
-  Tensor output_tensor = makeOutputTensor(DataType::S32);
-
-  LogicalNot kernel(&input_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(LogicalNotTest, InputTypeInvalid_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::S32>({1, 1, 1, 4}, {1, 0, 0, 1});
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  LogicalNot kernel(&input_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/LogicalOr.h b/compiler/luci-interpreter/src/kernels/LogicalOr.h
deleted file mode 100644
index 88606483f..000000000
--- a/compiler/luci-interpreter/src/kernels/LogicalOr.h
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2019 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_LOGICALOR_H
-#define LUCI_INTERPRETER_KERNELS_LOGICALOR_H
-
-#include "core/Kernel.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class LogicalOr : public Kernel
-{
-public:
-  LogicalOr(const Tensor *input1, const Tensor *input2, Tensor *output);
-
-  const Tensor *input1() const { return _inputs[0]; }
-  const Tensor *input2() const { return _inputs[1]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_LOGICALOR_H
diff --git a/compiler/luci-interpreter/src/kernels/MaxPool2D.test.cpp b/compiler/luci-interpreter/src/kernels/MaxPool2D.test.cpp
deleted file mode 100644
index b9991f7ec..000000000
--- a/compiler/luci-interpreter/src/kernels/MaxPool2D.test.cpp
+++ /dev/null
@@ -1,125 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/MaxPool2D.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(MaxPool2DTest, Float)
-{
-  Shape input_shape{1, 3, 5, 1};
-  std::vector<float> input_data{
-    1,  -1, 0,  -2, 2,  //
-    -7, -6, -5, -4, -3, //
-    5,  4,  3,  6,  7,  //
-  };
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  Pool2DParams params{};
-  params.padding = Padding::VALID;
-  params.filter_height = 2;
-  params.filter_width = 3;
-  params.stride_height = 1;
-  params.stride_width = 2;
-  params.activation = Activation::RELU6;
-
-  MaxPool2D kernel(&input_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  std::vector<float> ref_output_data{
-    1, 2, //
-    5, 6, //
-  };
-  std::initializer_list<int32_t> ref_output_shape{1, 2, 2, 1};
-  EXPECT_THAT(extractTensorData<float>(output_tensor), FloatArrayNear(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(ref_output_shape));
-}
-
-TEST(MaxPool2DTest, Uint8)
-{
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(-15.9375, 15.9375);
-  std::vector<float> input_data{
-    0,  -6, 12, 4, //
-    -3, -2, 10, 7, //
-  };
-  Tensor input_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, quant_param.first, quant_param.second, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::U8, quant_param.first, quant_param.second);
-
-  Pool2DParams params{};
-  params.padding = Padding::VALID;
-  params.filter_height = 2;
-  params.filter_width = 2;
-  params.stride_height = 2;
-  params.stride_width = 2;
-  params.activation = Activation::RELU6;
-
-  MaxPool2D kernel(&input_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  std::vector<float> ref_output_data{0.0, 6.0};
-  std::initializer_list<int32_t> ref_output_shape{1, 1, 2, 1};
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(ref_output_shape));
-}
-
-TEST(MaxPool2DTest, SInt16)
-{
-  Shape input_shape{1, 3, 5, 1};
-  std::vector<int32_t> ref_output_shape{1, 2, 2, 1};
-  std::vector<float> input_data{
-    1,  -1, 0,  -2, 2,  //
-    -7, -6, -5, -4, -3, //
-    5,  4,  3,  6,  7,  //
-  };
-  std::vector<float> ref_output_data{
-    1, 2, //
-    5, 6, //
-  };
-
-  Tensor input_tensor = makeInputTensor<DataType::S16>(input_shape, 0.2, 0, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.2, 0);
-
-  Pool2DParams params{};
-  params.padding = Padding::VALID;
-  params.filter_height = 2;
-  params.filter_width = 3;
-  params.stride_height = 1;
-  params.stride_width = 2;
-  params.activation = Activation::RELU6;
-
-  MaxPool2D kernel(&input_tensor, &output_tensor, params);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(ref_output_shape));
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Mean.cpp b/compiler/luci-interpreter/src/kernels/Mean.cpp
deleted file mode 100644
index 421632812..000000000
--- a/compiler/luci-interpreter/src/kernels/Mean.cpp
+++ /dev/null
@@ -1,331 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2019 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Mean.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/reference_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-static void resolveAxes(const int *axes_data, int num_axes, tflite::MeanParams *params)
-{
-  params->axis_count = num_axes;
-  for (int i = 0; i < num_axes; ++i)
-  {
-    params->axis[i] = static_cast<int16>(axes_data[i]);
-  }
-  for (int i = num_axes; i < 4; ++i)
-  {
-    params->axis[i] = 1;
-  }
-}
-
-// Returns the number of axes that will be reduced. Removes duplicates.
-static int getAxisReductionCount(const int *axes_data, int num_axes, int input_num_dims)
-{
-  int reduction_count = num_axes;
-  for (int i = 0; i < num_axes; ++i)
-  {
-    int current = axes_data[i] >= 0 ? axes_data[i] : axes_data[i] + input_num_dims;
-    assert(current >= 0 && current < input_num_dims);
-    for (int j = 0; j < i; j++)
-    {
-      int previous = axes_data[j] >= 0 ? axes_data[j] : axes_data[j] + input_num_dims;
-      // This checks for duplicate axis
-      if (current == previous)
-      {
-        --reduction_count;
-        break;
-      }
-    }
-  }
-  return reduction_count;
-}
-
-static Shape getOutputShape(const Shape &input_shape, const int *axes_data, int num_axes,
-                            bool keep_dims)
-{
-  int input_num_dims = input_shape.num_dims();
-  if (input_num_dims == 0)
-  {
-    return Shape(0);
-  }
-
-  if (keep_dims)
-  {
-    Shape output_shape(input_num_dims);
-    for (int idx = 0; idx < input_num_dims; ++idx)
-    {
-      bool is_axis = false;
-      for (int axis_idx = 0; axis_idx < num_axes; ++axis_idx)
-      {
-        if (axes_data[axis_idx] == idx || axes_data[axis_idx] + input_num_dims == idx)
-        {
-          is_axis = true;
-          break;
-        }
-      }
-      if (is_axis)
-      {
-        output_shape.dim(idx) = 1;
-      }
-      else
-      {
-        output_shape.dim(idx) = input_shape.dim(idx);
-      }
-    }
-    return output_shape;
-  }
-  else
-  {
-    int num_reduce_axes = getAxisReductionCount(axes_data, num_axes, input_num_dims);
-    Shape output_shape(input_num_dims - num_reduce_axes);
-    int num_skip_axes = 0;
-    for (int idx = 0; idx < input_num_dims; ++idx)
-    {
-      bool is_axis = false;
-      for (int axis_idx = 0; axis_idx < num_axes; ++axis_idx)
-      {
-        if (axes_data[axis_idx] == idx || axes_data[axis_idx] + input_num_dims == idx)
-        {
-          ++num_skip_axes;
-          is_axis = true;
-          break;
-        }
-      }
-      if (!is_axis)
-      {
-        output_shape.dim(idx - num_skip_axes) = input_shape.dim(idx);
-      }
-    }
-    return output_shape;
-  }
-}
-
-Mean::Mean(const Tensor *input, const Tensor *axes, Tensor *output, const ReducerParams &params)
-  : KernelWithParams<ReducerParams>({input, axes}, {output}, params)
-{
-}
-
-void Mean::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-  LUCI_INTERPRETER_CHECK(axes()->element_type() == DataType::S32);
-  if (input()->element_type() == DataType::S16)
-  {
-    LUCI_INTERPRETER_CHECK(input()->zero_point() == 0 && output()->zero_point() == 0);
-  }
-
-  const Shape &input_shape = input()->shape();
-  int input_num_dims = input_shape.num_dims();
-
-  const auto *axes_data = getTensorData<int32_t>(axes());
-  int num_axes = axes()->shape().num_elements();
-  assert(num_axes <= 4);
-
-  Shape output_shape = getOutputShape(input_shape, axes_data, num_axes, _params.keep_dims);
-  output()->resize(output_shape);
-
-  tflite::MeanParams params{};
-  resolveAxes(axes_data, num_axes, &params);
-  const bool need_temporaries = !(
-    _params.keep_dims && input_num_dims == 4 && params.axis_count == 2 &&
-    ((params.axis[0] == 1 && params.axis[1] == 2) || (params.axis[0] == 2 && params.axis[1] == 1)));
-  if (need_temporaries)
-  {
-    _temp_index =
-      std::make_unique<Tensor>(DataType::S32, Shape(input_num_dims), AffineQuantization{}, "");
-    _resolved_axes =
-      std::make_unique<Tensor>(DataType::S32, Shape(num_axes), AffineQuantization{}, "");
-    _temp_sum = std::make_unique<Tensor>(input()->element_type(), output()->shape(),
-                                         AffineQuantization{}, "");
-  }
-}
-
-void Mean::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::U8:
-      evalQuantized();
-      break;
-    case DataType::S16:
-      evalQuantizedS16();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-  if (!!_temp_index)
-    _temp_index->deallocate();
-  if (!!_resolved_axes)
-    _resolved_axes->deallocate();
-  if (!!_temp_sum)
-    _temp_sum->deallocate();
-}
-
-void Mean::evalFloat() const
-{
-  const Shape &input_shape = input()->shape();
-  int input_num_dims = input_shape.num_dims();
-  const auto *axes_data = getTensorData<int32_t>(axes());
-  int num_axes = axes()->shape().num_elements();
-
-  tflite::MeanParams params{};
-  resolveAxes(axes_data, num_axes, &params);
-
-  // Defer to specialized implementation for 4D Mean across axes 1 & 2.
-  if (_params.keep_dims && input_num_dims == 4 && params.axis_count == 2 &&
-      ((params.axis[0] == 1 && params.axis[1] == 2) ||
-       (params.axis[0] == 2 && params.axis[1] == 1)))
-  {
-    tflite::reference_ops::Mean(params, getTensorShape(input()), getTensorData<float>(input()),
-                                getTensorShape(output()), getTensorData<float>(output()));
-  }
-  else
-  {
-    tflite::reference_ops::Mean(
-      getTensorData<float>(input()), getTensorShape(input()).DimsData(),
-      input()->shape().num_dims(), getTensorData<float>(output()),
-      getTensorShape(output()).DimsData(), output()->shape().num_dims(), axes_data, num_axes,
-      _params.keep_dims, getTensorData<int>(_temp_index.get()),
-      getTensorData<int>(_resolved_axes.get()), getTensorData<float>(_temp_sum.get()));
-  }
-}
-
-void Mean::evalQuantized() const
-{
-  const Shape &input_shape = input()->shape();
-  int input_num_dims = input_shape.num_dims();
-  const auto *axes_data = getTensorData<int32_t>(axes());
-  int num_axes = axes()->shape().num_elements();
-
-  tflite::MeanParams params{};
-  resolveAxes(axes_data, num_axes, &params);
-
-  // Defer to specialized implementation for 4D Mean across axes 1 & 2.
-  if (_params.keep_dims && input_num_dims == 4 && params.axis_count == 2 &&
-      ((params.axis[0] == 1 && params.axis[1] == 2) ||
-       (params.axis[0] == 2 && params.axis[1] == 1)))
-  {
-    tflite::reference_ops::Mean(params, getTensorShape(input()), getTensorData<uint8_t>(input()),
-                                input()->zero_point(), input()->scale(), getTensorShape(output()),
-                                getTensorData<uint8_t>(output()), output()->zero_point(),
-                                output()->scale());
-  }
-  else if (input()->zero_point() == output()->zero_point() && input()->scale() == output()->scale())
-  {
-    tflite::reference_ops::Mean(
-      getTensorData<uint8_t>(input()), getTensorShape(input()).DimsData(),
-      input()->shape().num_dims(), getTensorData<uint8_t>(output()),
-      getTensorShape(output()).DimsData(), output()->shape().num_dims(), axes_data, num_axes,
-      _params.keep_dims, getTensorData<int>(_temp_index.get()),
-      getTensorData<int>(_resolved_axes.get()), getTensorData<int>(_temp_sum.get()));
-  }
-  else
-  {
-    tflite::reference_ops::QuantizedMeanOrSum<>(
-      getTensorData<uint8_t>(input()), input()->zero_point(), input()->scale(),
-      getTensorShape(input()).DimsData(), input()->shape().num_dims(),
-      getTensorData<uint8_t>(output()), output()->zero_point(), output()->scale(),
-      getTensorShape(output()).DimsData(), output()->shape().num_dims(), axes_data, num_axes,
-      _params.keep_dims, getTensorData<int>(_temp_index.get()),
-      getTensorData<int>(_resolved_axes.get()), getTensorData<int>(_temp_sum.get()),
-      /*compute_sum=*/false);
-  }
-}
-
-void Mean::evalQuantizedS16() const
-{
-  const auto *input_data = getTensorData<int16_t>(input());
-  auto *output_data = getTensorData<int16_t>(output());
-
-  const Shape &input_shape = input()->shape();
-  const Shape &output_shape = output()->shape();
-
-  const auto *axes_data = getTensorData<int32_t>(axes());
-  const int num_axes = axes()->shape().num_elements();
-
-  constexpr int32_t output_min = -std::numeric_limits<int16_t>::max();
-  constexpr int32_t output_max = std::numeric_limits<int16_t>::max();
-
-  // Defer to specialized implementation for 4D Mean across axes 1 & 2.
-  if (_params.keep_dims && input_shape.num_dims() == 4 && num_axes == 2 &&
-      ((axes_data[0] == 1 && axes_data[1] == 2) || (axes_data[0] == 2 && axes_data[1] == 1)))
-  {
-    const int32_t batches = input_shape.dim(0);
-    const int32_t input_height = input_shape.dim(1);
-    const int32_t input_width = input_shape.dim(2);
-    const int32_t depth = input_shape.dim(3);
-    assert(output_shape.num_dims() == 4);
-    assert(output_shape.dim(0) == batches);
-    assert(output_shape.dim(1) == 1);
-    assert(output_shape.dim(2) == 1);
-    assert(output_shape.dim(3) == depth);
-
-    const double real_multiplier =
-      static_cast<double>(input()->scale()) / static_cast<double>(output()->scale());
-
-    int32_t output_multiplier{};
-    int output_shift{};
-    quantizeMultiplier(real_multiplier, &output_multiplier, &output_shift);
-
-    const int32_t num_elements_in_axes = input_height * input_width;
-
-    for (int32_t batch = 0; batch < batches; ++batch)
-    {
-      for (int32_t c = 0; c < depth; ++c)
-      {
-        int32_t acc = 0;
-        for (int32_t in_y = 0; in_y < input_height; ++in_y)
-        {
-          for (int32_t in_x = 0; in_x < input_width; ++in_x)
-          {
-            acc += input_data[calcOffset(input_shape, batch, in_y, in_x, c)];
-          }
-        }
-        int32_t scaled_acc =
-          tflite::MultiplyByQuantizedMultiplier(acc, output_multiplier, output_shift);
-        // Divide by the number of elements rounding to the nearest integer.
-        scaled_acc = scaled_acc > 0
-                       ? (scaled_acc + num_elements_in_axes / 2) / num_elements_in_axes
-                       : (scaled_acc - num_elements_in_axes / 2) / num_elements_in_axes;
-
-        scaled_acc = std::max(scaled_acc, output_min);
-        scaled_acc = std::min(scaled_acc, output_max);
-
-        output_data[calcOffset(output_shape, batch, 0, 0, c)] = scaled_acc;
-      }
-    }
-  }
-  else
-  {
-    throw std::runtime_error("Unsupported configuration.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Minimum.cpp b/compiler/luci-interpreter/src/kernels/Minimum.cpp
deleted file mode 100644
index 5d3dcde72..000000000
--- a/compiler/luci-interpreter/src/kernels/Minimum.cpp
+++ /dev/null
@@ -1,65 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2018 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Minimum.h"
-
-#include "kernels/Utils.h"
-
-#include "kernels/BinaryOpCommon.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-Minimum::Minimum(const Tensor *input1, const Tensor *input2, Tensor *output)
-  : Kernel({input1, input2}, {output})
-{
-}
-
-void Minimum::configure()
-{
-  LUCI_INTERPRETER_CHECK(input1()->element_type() == input2()->element_type())
-  LUCI_INTERPRETER_CHECK(input1()->element_type() == output()->element_type())
-  output()->resize(calculateShapeForBroadcast(input1()->shape(), input2()->shape()));
-}
-
-void Minimum::execute() const
-{
-  switch (input1()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalMinimum<float>();
-      break;
-    case DataType::U8:
-      evalMinimum<uint8_t>();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-template <typename T> inline void Minimum::evalMinimum() const
-{
-  BinaryOpBroadcastSlow(getTensorShape(input1()), getTensorData<T>(input1()),
-                        getTensorShape(input2()), getTensorData<T>(input2()),
-                        getTensorShape(output()), getTensorData<T>(output()),
-                        [](T x, T y) { return std::min(x, y); });
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Minimum.h b/compiler/luci-interpreter/src/kernels/Minimum.h
deleted file mode 100644
index 5ff4035b4..000000000
--- a/compiler/luci-interpreter/src/kernels/Minimum.h
+++ /dev/null
@@ -1,47 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_MINIMUM_H
-#define LUCI_INTERPRETER_KERNELS_MINIMUM_H
-
-#include "core/Kernel.h"
-#include "core/KernelParams.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Minimum : public Kernel
-{
-public:
-  Minimum(const Tensor *input1, const Tensor *input2, Tensor *output);
-
-  const Tensor *input1() const { return _inputs[0]; }
-  const Tensor *input2() const { return _inputs[1]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  template <typename T> inline void evalMinimum() const;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_MINIMUM_H
diff --git a/compiler/luci-interpreter/src/kernels/Mul.h b/compiler/luci-interpreter/src/kernels/Mul.h
deleted file mode 100644
index 2ccf60f3a..000000000
--- a/compiler/luci-interpreter/src/kernels/Mul.h
+++ /dev/null
@@ -1,51 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_MUL_H
-#define LUCI_INTERPRETER_KERNELS_MUL_H
-
-#include "core/Kernel.h"
-#include "core/KernelParams.h"
-
-#include <cstdint>
-#include <vector>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Mul : public KernelWithParams<MulParams>
-{
-public:
-  Mul(const Tensor *input1, const Tensor *input2, Tensor *output, const MulParams &params);
-
-  const Tensor *input1() const { return _inputs[0]; }
-  const Tensor *input2() const { return _inputs[1]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  void evalFloat() const;
-  void evalQuantizedS16() const;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_MUL_H
diff --git a/compiler/luci-interpreter/src/kernels/NotEqual.cpp b/compiler/luci-interpreter/src/kernels/NotEqual.cpp
deleted file mode 100644
index 99d5e0fa0..000000000
--- a/compiler/luci-interpreter/src/kernels/NotEqual.cpp
+++ /dev/null
@@ -1,113 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/NotEqual.h"
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/comparisons.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-NotEqual::NotEqual(const Tensor *x, const Tensor *y, Tensor *output) : Kernel({x, y}, {output}) {}
-
-void NotEqual::configure()
-{
-  LUCI_INTERPRETER_CHECK(x()->element_type() == y()->element_type());
-  LUCI_INTERPRETER_CHECK(output()->element_type() == DataType::BOOL);
-
-  if (x()->element_type() == DataType::U8)
-  {
-    quantizeMultiplierSmallerThanOneExp(x()->scale(), &_x_multiplier, &_x_shift);
-    quantizeMultiplierSmallerThanOneExp(y()->scale(), &_y_multiplier, &_y_shift);
-  }
-  output()->resize(calculateShapeForBroadcast(x()->shape(), y()->shape()));
-}
-
-void NotEqual::execute() const
-{
-  switch (x()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::U8:
-      evalQuantized();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void NotEqual::evalFloat() const
-{
-  const auto x_data = getTensorData<float>(x());
-  const auto y_data = getTensorData<float>(y());
-  auto output_data = getTensorData<bool>(output());
-
-  tflite::ComparisonParams op_params;
-  op_params.is_broadcast = x()->shape() != y()->shape();
-
-  if (op_params.is_broadcast)
-  {
-    tflite::reference_ops::Broadcast4DSlowNotEqual(op_params, getTensorShape(x()), x_data,
-                                                   getTensorShape(y()), y_data,
-                                                   getTensorShape(output()), output_data);
-  }
-  else
-  {
-    tflite::reference_ops::NotEqual(op_params, getTensorShape(x()), x_data, getTensorShape(y()),
-                                    y_data, getTensorShape(output()), output_data);
-  }
-}
-
-void NotEqual::evalQuantized() const
-{
-  const auto x_data = getTensorData<uint8_t>(x());
-  const auto y_data = getTensorData<uint8_t>(y());
-  auto output_data = getTensorData<bool>(output());
-
-  tflite::ComparisonParams op_params;
-  op_params.left_shift = 8;
-  op_params.input1_offset = -x()->zero_point(); // Note the '-'
-  op_params.input1_shift = _x_shift;
-  op_params.input1_multiplier = _x_multiplier;
-  op_params.input2_offset = -y()->zero_point(); // Note the '-'
-  op_params.input2_shift = _y_shift;
-  op_params.input2_multiplier = _y_multiplier;
-  op_params.is_broadcast = x()->shape() != y()->shape();
-
-  if (op_params.is_broadcast)
-  {
-    tflite::reference_ops::Broadcast4DSlowNotEqualWithScaling(
-      op_params, getTensorShape(x()), x_data, getTensorShape(y()), y_data, getTensorShape(output()),
-      output_data);
-  }
-  else
-  {
-    tflite::reference_ops::NotEqualWithScaling(op_params, getTensorShape(x()), x_data,
-                                               getTensorShape(y()), y_data,
-                                               getTensorShape(output()), output_data);
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/NotEqual.test.cpp b/compiler/luci-interpreter/src/kernels/NotEqual.test.cpp
deleted file mode 100644
index f9dc7781b..000000000
--- a/compiler/luci-interpreter/src/kernels/NotEqual.test.cpp
+++ /dev/null
@@ -1,187 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/NotEqual.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(NotEqualTest, FloatSimple)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-    -1,  0,   1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    true, false, true, // Row 1
-    true, false, true, // Row 2
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({2, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  NotEqual kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({2, 3}));
-}
-
-TEST(NotEqualTest, FloatBroardcast)
-{
-  std::vector<float> x_data{
-    0.5, 0.7, 0.9, // Row 1
-    1,   0,   -1,  // Row 2
-    -1,  0,   1,   // Row 3
-    0.9, 0.7, 0.5, // Row 4
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.7, 0.5, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    true,  false, true,  // Row 1
-    true,  true,  true,  // Row 2
-    true,  true,  true,  // Row 3
-    false, false, false, // Row 4
-  };
-
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({4, 3}, x_data);
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1, 3}, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  NotEqual kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({4, 3}));
-}
-
-// Choose min / max in such a way that there are exactly 256 units to avoid rounding errors.
-const float F_MIN = -128.0 / 128.0;
-const float F_MAX = 127.0 / 128.0;
-
-TEST(NotEqualTest, Uint8Quantized)
-{
-  std::vector<float> x_data{
-    0.5, 0.5, 0.7,  0.9, // Row 1
-    1,   0,   0.05, -1,  // Row 2
-  };
-
-  std::vector<float> y_data{
-    0.9, 0.5, 0.55, 0.5, // Row 1
-    -1,  0,   0.05, 1,   // Row 2
-  };
-
-  std::vector<bool> ref_output_data{
-    true, false, true,  true, // Row 1
-    true, false, false, true, // Row 2
-  };
-
-  std::pair<float, int32_t> x_quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, x_quant_param.first, x_quant_param.second, x_data);
-
-  std::pair<float, int32_t> y_quant_param = quantizationParams<uint8_t>(F_MIN * 2, F_MAX * 2);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 4, 1}, y_quant_param.first, y_quant_param.second, y_data);
-
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  NotEqual kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(NotEqualTest, Uint8QuantizedBroadcast)
-{
-  std::vector<float> x_data{
-    0.4,  -0.8, 0.7,  0.3, // Row 1
-    -0.5, 0.1,  0,    0.5, // Row 2
-    1,    0,    0.05, -1,  // Row 3
-    -1,   0.05, 0,    1,   // Row 4
-  };
-
-  std::vector<float> y_data{
-    -1, 0.05, 0, 1, // Row 1
-  };
-
-  std::vector<bool> ref_output_data{
-    true,  true,  true,  true,  // Row 1
-    true,  true,  false, true,  // Row 2
-    true,  true,  true,  true,  // Row 3
-    false, false, false, false, // Row 4
-  };
-
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(F_MIN, F_MAX);
-  Tensor x_tensor =
-    makeInputTensor<DataType::U8>({1, 4, 4, 1}, quant_param.first, quant_param.second, x_data);
-  Tensor y_tensor =
-    makeInputTensor<DataType::U8>({1, 1, 4, 1}, quant_param.first, quant_param.second, y_data);
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  NotEqual kernel(&x_tensor, &y_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 4, 4, 1}));
-  EXPECT_THAT(extractTensorData<bool>(output_tensor), ::testing::ElementsAreArray(ref_output_data));
-}
-
-TEST(NotEqualTest, Input_Type_Mismatch_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::U8>({1}, {1});
-  Tensor output_tensor = makeOutputTensor(DataType::BOOL);
-
-  NotEqual kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(NotEqualTest, Input_Output_Type_NEG)
-{
-  Tensor x_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor y_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  NotEqual kernel(&x_tensor, &y_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Pack.test.cpp b/compiler/luci-interpreter/src/kernels/Pack.test.cpp
deleted file mode 100644
index 092bd449a..000000000
--- a/compiler/luci-interpreter/src/kernels/Pack.test.cpp
+++ /dev/null
@@ -1,144 +0,0 @@
-/*
- * Copyright (c) 2021 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Pack.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-template <typename T>
-void Check(std::vector<std::initializer_list<int32_t>> input_shapes,
-           std::initializer_list<int32_t> output_shape, std::vector<std::vector<T>> input_datas,
-           std::initializer_list<T> output_data, int32_t axis)
-{
-  constexpr DataType element_type = getElementType<T>();
-  std::vector<const Tensor *> inputs(input_datas.size());
-  std::vector<Tensor> tmp_inputs;
-  for (int i = 0; i < input_datas.size(); i++)
-  {
-    if (std::is_same<T, float>::value)
-    {
-      tmp_inputs.push_back(Tensor(element_type, input_shapes[i], {}, ""));
-      tmp_inputs[i].writeData(input_datas[i].data(), input_datas[i].size() * sizeof(T));
-    }
-    else
-    {
-      tmp_inputs.push_back(Tensor(element_type, input_shapes[i], {{1.0f / 255}, {128}}, ""));
-      tmp_inputs[i].writeData(input_datas[i].data(), input_datas[i].size() * sizeof(T));
-    }
-  }
-  for (int i = 0; i < input_datas.size(); i++)
-  {
-    inputs[i] = &tmp_inputs[i];
-  }
-
-  Tensor output_tensor = makeOutputTensor(element_type);
-  if (!std::is_same<T, float>::value)
-  {
-    output_tensor = makeOutputTensor(element_type, 1.0f / 255, 128);
-  }
-
-  PackParams params{};
-  params.axis = axis;
-  params.values_count = input_datas.size();
-  Pack kernel(inputs, &output_tensor, params);
-
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<T>(output_tensor), ::testing::ElementsAreArray(output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(output_shape));
-}
-
-template <typename T> class PackTest : public ::testing::Test
-{
-};
-
-using DataTypes = ::testing::Types<uint8_t, float>;
-TYPED_TEST_CASE(PackTest, DataTypes);
-
-TYPED_TEST(PackTest, ThreeInputs)
-{
-  Check<TypeParam>(/*input_shapes=*/{{2}, {2}, {2}},
-                   /*output_shape=*/{3, 2},
-                   /*input_datas=*/
-                   {{1, 4}, {2, 5}, {3, 6}},
-                   /*output_data=*/
-                   {1, 4, 2, 5, 3, 6}, /*axis=*/0);
-
-  SUCCEED();
-}
-
-TYPED_TEST(PackTest, NegAxis)
-{
-  Check<TypeParam>(/*input_shapes=*/{{2}, {2}, {2}},
-                   /*output_shape=*/{2, 3},
-                   /*input_datas=*/
-                   {{1, 4}, {2, 5}, {3, 6}},
-                   /*output_data=*/
-                   {1, 2, 3, 4, 5, 6}, /*axis=*/-1);
-
-  SUCCEED();
-}
-
-TEST(Pack, MismatchingInputValuesCount_NEG)
-{
-  std::vector<float> input1_data{1, 4};
-  std::vector<float> input2_data{2, 5};
-  std::vector<float> input3_data{3, 6};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::FLOAT32>({2}, input2_data);
-  Tensor input3_tensor = makeInputTensor<DataType::FLOAT32>({2}, input3_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  PackParams params{};
-  {
-    params.axis = 0;
-    params.values_count = 2;
-
-    Pack kernel({&input1_tensor, &input2_tensor, &input3_tensor}, &output_tensor, params);
-    EXPECT_ANY_THROW(kernel.configure());
-  }
-}
-
-TEST(Pack, InvalidInputAxis_NEG)
-{
-  std::vector<float> input1_data{1, 4};
-  std::vector<float> input2_data{2, 5};
-  std::vector<float> input3_data{3, 6};
-  Tensor input1_tensor = makeInputTensor<DataType::FLOAT32>({2}, input1_data);
-  Tensor input2_tensor = makeInputTensor<DataType::FLOAT32>({2}, input2_data);
-  Tensor input3_tensor = makeInputTensor<DataType::FLOAT32>({2}, input3_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-  PackParams params{};
-  {
-    params.axis = 2;
-    params.values_count = 3;
-
-    Pack kernel({&input1_tensor, &input2_tensor, &input3_tensor}, &output_tensor, params);
-    EXPECT_ANY_THROW(kernel.configure());
-  }
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/PadV2.cpp b/compiler/luci-interpreter/src/kernels/PadV2.cpp
deleted file mode 100644
index 3c215dbca..000000000
--- a/compiler/luci-interpreter/src/kernels/PadV2.cpp
+++ /dev/null
@@ -1,106 +0,0 @@
-/*
- * Copyright (c) 2021 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/PadV2.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/reference_ops.h>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-PadV2::PadV2(const Tensor *input, const Tensor *paddings, const Tensor *constant_values,
-             Tensor *output)
-  : Kernel({input, paddings, constant_values}, {output})
-{
-}
-
-void PadV2::configure()
-{
-  const Shape &input_shape = input()->shape();
-  const int num_dims = input_shape.num_dims();
-
-  if (num_dims > 4)
-    throw std::runtime_error("Unsupported number of dimensions.");
-
-  assert(output()->element_type() == input()->element_type());
-  assert(paddings()->element_type() == DataType::S32);
-  assert(constant_values()->element_type() == output()->element_type());
-  // Paddings shape should be [N, 2].
-  assert(paddings()->shape().num_dims() == 2);
-  assert(paddings()->shape().dim(0) == num_dims);
-  assert(paddings()->shape().dim(1) == 2);
-  // Constant values elements number should be 1.
-  assert(constant_values()->shape().num_elements() == 1);
-
-  Shape output_shape(num_dims);
-  const auto *paddings_data = getTensorData<int32_t>(paddings());
-  for (int i = 0; i < num_dims; ++i)
-  {
-    const int32_t padding_before = paddings_data[i * 2];
-    const int32_t padding_after = paddings_data[i * 2 + 1];
-    assert(padding_before >= 0 && padding_after >= 0);
-    output_shape.dim(i) = input_shape.dim(i) + padding_before + padding_after;
-  }
-
-  output()->resize(output_shape);
-}
-
-void PadV2::execute() const
-{
-  const int num_dims = input()->shape().num_dims();
-
-  tflite::PadParams params{};
-  params.left_padding_count = num_dims;
-  params.right_padding_count = num_dims;
-
-  const auto *paddings_data = getTensorData<int32_t>(paddings());
-  for (int i = num_dims - 1; i >= 0; --i)
-  {
-    params.left_padding[i] = paddings_data[i * 2];
-    params.right_padding[i] = paddings_data[i * 2 + 1];
-  }
-
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-    {
-      const auto pad_value = getTensorData<float>(constant_values())[0];
-      tflite::reference_ops::Pad(params, getTensorShape(input()), getTensorData<float>(input()),
-                                 &pad_value, getTensorShape(output()),
-                                 getTensorData<float>(output()));
-      break;
-    }
-    case DataType::U8:
-    {
-      assert(output()->zero_point() >= std::numeric_limits<uint8_t>::min());
-      assert(output()->zero_point() <= std::numeric_limits<uint8_t>::max());
-      const auto pad_value = getTensorData<uint8_t>(constant_values())[0];
-      tflite::reference_ops::Pad(params, getTensorShape(input()), getTensorData<uint8_t>(input()),
-                                 &pad_value, getTensorShape(output()),
-                                 getTensorData<uint8_t>(output()));
-      break;
-    }
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Prelu.cpp b/compiler/luci-interpreter/src/kernels/Prelu.cpp
deleted file mode 100644
index c4b288f1b..000000000
--- a/compiler/luci-interpreter/src/kernels/Prelu.cpp
+++ /dev/null
@@ -1,209 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Prelu.h"
-
-#include "kernels/BinaryOpCommon.h"
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/reference_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-Prelu::Prelu(const Tensor *input, const Tensor *alpha, Tensor *output)
-  : Kernel({input, alpha}, {output})
-{
-}
-
-Prelu::~Prelu()
-{
-  // Destructor declared to delete vector of alpha quantized data properly
-}
-
-void Prelu::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-  LUCI_INTERPRETER_CHECK(alpha()->element_type() == output()->element_type());
-  LUCI_INTERPRETER_CHECK(input()->scales().size() <= 1);
-  LUCI_INTERPRETER_CHECK(output()->scales().size() <= 1);
-
-  if (input()->element_type() == DataType::U8)
-  {
-    LUCI_INTERPRETER_CHECK(alpha()->scales().size() <= 1); // remove when CWQ kernel arrives
-    _alpha_multipliers.resize(1);
-    double alpha_multiplier = input()->scale() * alpha()->scale() / output()->scale();
-    quantizeMultiplier(alpha_multiplier, &_alpha_multipliers[0].multiplier,
-                       &_alpha_multipliers[0].shift);
-    double identity_multiplier = input()->scale() / output()->scale();
-    quantizeMultiplier(identity_multiplier, &_output_multiplier_identity, &_output_shift_identity);
-  }
-  else if (input()->element_type() == DataType::S16)
-  {
-    // Common check for correctness of quant params
-    LUCI_INTERPRETER_CHECK(input()->zero_point() == 0 && output()->zero_point() == 0);
-    for (size_t channel = 0; channel < alpha()->zero_points().size(); ++channel)
-    {
-      LUCI_INTERPRETER_CHECK(alpha()->zero_points()[channel] == 0);
-    }
-    // Prelu specific checks for CWQ
-    LUCI_INTERPRETER_CHECK(alpha()->quantized_dimension() == alpha()->shape().num_dims() - 1);
-    LUCI_INTERPRETER_CHECK(static_cast<int32_t>(alpha()->scales().size()) ==
-                           alpha()->shape().dim(alpha()->quantized_dimension()));
-    LUCI_INTERPRETER_CHECK(alpha()->shape().num_elements() ==
-                           input()->shape().dim(input()->shape().num_dims() - 1));
-
-    // all dimension of alpha except last one should be size 1
-    for (int dim = 0; dim < alpha()->shape().num_dims() - 1; ++dim)
-    {
-      LUCI_INTERPRETER_CHECK(alpha()->shape().dim(dim) == 1);
-    }
-
-    std::vector<double> real_multipliers =
-      getQuantizedConvolutionMultiplers(input()->scale(), alpha()->scales(), output()->scale());
-
-    _alpha_multipliers = quantizeMultipliers(real_multipliers);
-
-    double identity_multiplier = input()->scale() / output()->scale();
-    quantizeMultiplier(identity_multiplier, &_output_multiplier_identity, &_output_shift_identity);
-  }
-  output()->resize(calculateShapeForBroadcast(input()->shape(), alpha()->shape()));
-}
-
-void Prelu::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::U8:
-      evalQuantized();
-      break;
-    case DataType::S16:
-      evalQuantizedS16();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void Prelu::evalFloat() const
-{
-  const auto input_data = getTensorData<float>(input());
-  const auto alpha_data = getTensorData<float>(alpha());
-  const auto size = getTensorShape(input()).FlatSize();
-  auto output_data = getTensorData<float>(output());
-
-  auto PreluFunc = [](float input, float alpha) { return input >= 0.0 ? input : input * alpha; };
-
-  if (input()->shape() != alpha()->shape())
-  {
-    tflite::reference_ops::BroadcastBinaryFunction4DSlow<float, float, float>(
-      getTensorShape(input()), getTensorData<float>(input()), getTensorShape(alpha()),
-      getTensorData<float>(alpha()), getTensorShape(output()), getTensorData<float>(output()),
-      PreluFunc);
-  }
-  else
-  {
-    for (auto i = decltype(size){0}; i < size; ++i)
-    {
-      if (input_data[i] >= 0)
-        output_data[i] = input_data[i];
-      else
-        output_data[i] = input_data[i] * alpha_data[i];
-    }
-  }
-}
-
-void Prelu::evalQuantized() const
-{
-  tflite::PreluParams op_params{};
-
-  op_params.input_offset = -input()->zero_point(); // Note the '-'.
-  op_params.alpha_offset = -alpha()->zero_point(); // Note the '-'.
-  op_params.output_offset = output()->zero_point();
-  op_params.output_shift_1 = _output_shift_identity;
-  op_params.output_multiplier_1 = _output_multiplier_identity;
-  op_params.output_shift_2 = _alpha_multipliers[0].shift;
-  op_params.output_multiplier_2 = _alpha_multipliers[0].multiplier;
-
-  if (input()->shape() != alpha()->shape())
-  {
-    tflite::reference_ops::BroadcastPrelu4DSlow(
-      op_params, getTensorShape(input()), getTensorData<uint8_t>(input()), getTensorShape(alpha()),
-      getTensorData<uint8_t>(alpha()), getTensorShape(output()), getTensorData<uint8_t>(output()));
-  }
-  else
-  {
-    tflite::reference_ops::Prelu<uint8_t>(
-      op_params, getTensorShape(input()), getTensorData<uint8_t>(input()), getTensorShape(alpha()),
-      getTensorData<uint8_t>(alpha()), getTensorShape(output()), getTensorData<uint8_t>(output()));
-  }
-}
-
-static inline int16_t evalElemS16Prelu(int16_t input_val, int16_t alpha_val,
-                                       const ChannelQuantMultipliers &identity_mult,
-                                       const ChannelQuantMultipliers &alpha_mult)
-{
-  constexpr int32_t quantized_min = std::numeric_limits<int16_t>::min();
-  constexpr int32_t quantized_max = std::numeric_limits<int16_t>::max();
-
-  const int32_t output_val =
-    input_val >= 0 ? tflite::MultiplyByQuantizedMultiplier(input_val, identity_mult.multiplier,
-                                                           identity_mult.shift)
-                   : tflite::MultiplyByQuantizedMultiplier(input_val * alpha_val,
-                                                           alpha_mult.multiplier, alpha_mult.shift);
-  const int32_t clamped_output = std::min(quantized_max, std::max(quantized_min, output_val));
-  return clamped_output;
-}
-
-void Prelu::evalQuantizedS16() const
-{
-  // Note that this kernel assumes alpha is CWQ
-  tflite::RuntimeShape input_shape = getTensorShape(input());
-  const int16_t *input_data = input()->data<int16_t>();
-  const int16_t *alpha_data = alpha()->data<int16_t>();
-  int16_t *output_data = output()->data<int16_t>();
-
-  const ChannelQuantMultipliers pos_mult{_output_shift_identity, _output_multiplier_identity};
-
-  const int last_dim = input()->shape().num_dims() - 1;
-
-  int32_t outer_dims_size = 1;
-  for (int i = 0; i < last_dim; ++i)
-    outer_dims_size *= input_shape.Dims(i);
-  int32_t quant_dim_size = input_shape.Dims(last_dim);
-
-  for (int32_t outer_dims = 0; outer_dims < outer_dims_size; ++outer_dims)
-    for (int32_t quant_channel = 0; quant_channel < quant_dim_size; ++quant_channel)
-    {
-      const ChannelQuantMultipliers &neg_mult = _alpha_multipliers[quant_channel];
-      size_t offset = static_cast<size_t>(outer_dims) * static_cast<size_t>(quant_dim_size);
-      offset += quant_channel;
-
-      output_data[offset] =
-        evalElemS16Prelu(input_data[offset], alpha_data[quant_channel], pos_mult, neg_mult);
-    }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Prelu.test.cpp b/compiler/luci-interpreter/src/kernels/Prelu.test.cpp
deleted file mode 100644
index 9d9adf66f..000000000
--- a/compiler/luci-interpreter/src/kernels/Prelu.test.cpp
+++ /dev/null
@@ -1,355 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Prelu.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-template <typename T>
-void Check(std::initializer_list<int32_t> input_shape, std::initializer_list<int32_t> alpha_shape,
-           std::initializer_list<int32_t> output_shape, std::initializer_list<T> input_data,
-           std::initializer_list<T> alpha_data, std::initializer_list<T> output_data)
-{
-  constexpr DataType element_type = getElementType<T>();
-  Tensor input_tensor = makeInputTensor<element_type>(input_shape, input_data);
-  Tensor alpha_tensor = makeInputTensor<element_type>(alpha_shape, alpha_data);
-  Tensor output_tensor = makeOutputTensor(element_type);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<T>(output_tensor), ::testing::ElementsAreArray(output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(output_shape));
-}
-
-TEST(PreluTest, FloatSimple)
-{
-  Check<float>(/*input_shape=*/{2, 3}, /*alpha_shape=*/{2, 3},
-               /*output_shape=*/{2, 3},
-               /*input_data=*/
-               {
-                 0.0f, 1.0f, 3.0f,   // Row 1
-                 1.0f, -1.0f, -2.0f, // Row 2
-               },
-               /*alpha_data=*/
-               {
-                 0.0f, 0.5f, 0.1f, // Row 1
-                 0.0f, 0.5f, 0.1f, // Row 2
-               },
-               /*output_data=*/
-               {
-                 0.0f, 1.0f, 3.0f,   // Row 1
-                 1.0f, -0.5f, -0.2f, // Row 2
-               });
-
-  SUCCEED();
-}
-
-TEST(PreluTest, FloatBroadcast)
-{
-  Check<float>(/*input_shape=*/{1, 2, 2, 3}, /*alpha_shape=*/{1, 1, 3},
-               /*output_shape=*/{1, 2, 2, 3},
-               /*input_data=*/
-               {
-                 0.0f, 0.0f, 0.0f,    // Row 1, Column 1
-                 1.0f, 1.0f, 1.0f,    // Row 1, Column 2
-                 -1.0f, -1.0f, -1.0f, // Row 2, Column 1
-                 -2.0f, -2.0f, -2.0f, // Row 2, Column 2
-               },
-               /*alpha_data=*/
-               {0.0f, 1.0f, 2.0f},
-               /*output_data=*/
-               {
-                 0.0f, 0.0f, 0.0f,   // Row 1, Column 1
-                 1.0f, 1.0f, 1.0f,   // Row 1, Column 2
-                 0.0f, -1.0f, -2.0f, // Row 2, Column 1
-                 0.0f, -2.0f, -4.0f, // Row 2, Column 2
-               });
-
-  SUCCEED();
-}
-
-float GetTolerance(float min, float max) { return (max - min) / 255.0; }
-
-TEST(PreluTest, Uint8Simple)
-{
-  std::vector<float> input_data{-0.8f, 0.2f, 0.9f, 0.7f, 0.1f, -0.4f};
-  std::vector<float> alpha_data{0.5f, 0.5f, 0.5f, 0.25f, 1.0f, 0.25f};
-  std::vector<float> ref_output_data{-0.4f, 0.2f, 0.9f, 0.7f, 0.1f, -0.1f};
-
-  float kQuantizedTolerance = GetTolerance(-1.0, 1.0);
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(-1.0f, 1.0f);
-
-  Tensor input_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 3, 1}, quant_param.first, quant_param.second, input_data);
-  Tensor alpha_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 3, 1}, quant_param.first, quant_param.second, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::U8, quant_param.first, quant_param.second);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(dequantizeTensorData(output_tensor),
-              FloatArrayNear(ref_output_data, kQuantizedTolerance));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 3, 1}));
-
-  SUCCEED();
-}
-
-TEST(PreluTest, Uint8Broadcast)
-{
-  std::vector<float> input_data{
-    0.0f,   0.0f,   0.0f,   // Row 1, Column 1
-    0.5f,   0.5f,   0.5f,   // Row 1, Column 2
-    -1.0f,  -1.0f,  -1.0f,  // Row 2, Column 1
-    -0.25f, -0.25f, -0.25f, // Row 2, Column 2
-  };
-  std::vector<float> alpha_data{0.0f, 0.5f, -0.5f};
-  std::vector<float> ref_output_data{
-    0.0f, 0.0f,    0.0f,  // Row 1, Column 1
-    0.5f, 0.5f,    0.5f,  // Row 1, Column 2
-    0.0f, -0.5f,   0.5f,  // Row 2, Column 1
-    0.0f, -0.125f, 0.125f // Row 2, Column 2
-  };
-  std::vector<float> ref_quant_output_data{
-    128, 128, 128, // Row 1, Column 1
-    192, 192, 192, // Row 1, Column 2
-    128, 64,  192, // Row 2, Column 1
-    128, 112, 144  // Row 2, Column 2
-  };
-  float kQuantizedTolerance = 2 * (1. / 256);
-  const float kMin = -1;
-  const float kMax = 127.f / 128.f;
-  std::pair<float, int32_t> quant_param = quantizationParams<uint8_t>(kMin, kMax);
-
-  Tensor input_tensor =
-    makeInputTensor<DataType::U8>({1, 2, 2, 3}, quant_param.first, quant_param.second, input_data);
-  Tensor alpha_tensor =
-    makeInputTensor<DataType::U8>({1, 1, 3}, quant_param.first, quant_param.second, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::U8, quant_param.first, quant_param.second);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(dequantizeTensorData(output_tensor),
-              FloatArrayNear(ref_output_data, kQuantizedTolerance));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 2, 3}));
-  EXPECT_THAT(extractTensorData<uint8_t>(output_tensor),
-              ::testing::ElementsAreArray(ref_quant_output_data));
-}
-
-TEST(PreluTest, SInt16_LWQ_NEG)
-{
-  // Rewrite this test in case layer-wise quantization for sint16 is supported
-  std::vector<float> input_data(6); // data is not important
-  std::vector<float> alpha_data(6);
-
-  Tensor input_tensor = makeInputTensor<DataType::S16>({1, 2, 3, 1}, 0.1, 0, input_data);
-  Tensor alpha_tensor = makeInputTensor<DataType::S16>({1, 2, 3, 1}, 0.1, 0, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.1, 0);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(PreluTest, SInt16_CWQ_Simple)
-{
-  std::vector<float> input_data{-0.8f, 0.2f, 0.9f, -0.7f, 0.1f, -0.4f};
-  std::vector<float> alpha_data{0.5f, 0.25f};
-  std::vector<float> ref_output_data{-0.4f, 0.2f, 0.9f, -0.175f, 0.1f, -0.1f};
-
-  std::vector<float> alpha_scales{0.05f, 0.025f};
-  std::vector<int32_t> zerop{0, 0};
-  Tensor input_tensor = makeInputTensor<DataType::S16>({1, 1, 3, 2}, 0.1, 0, input_data);
-  Tensor alpha_tensor = makeInputTensor<DataType::S16>({2}, alpha_scales, zerop, 0, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.025, 0);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 1, 3, 2}));
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-TEST(PreluTest, SInt16_CWQ_spatial_alpha_NEG)
-{
-  std::vector<float> input_data(6); // data is not important
-  std::vector<float> alpha_data(6);
-
-  std::vector<float> alpha_scales{0.25f, 0.05f};
-  std::vector<int32_t> zerop{0, 0};
-  Tensor input_tensor = makeInputTensor<DataType::S16>({1, 1, 3, 2}, 0.1, 0, input_data);
-  Tensor alpha_tensor =
-    makeInputTensor<DataType::S16>({1, 1, 3, 2}, alpha_scales, zerop, 3, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.1, 0);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(PreluTest, SInt16_CWQ_wrong_dim_quant_NEG)
-{
-  std::vector<float> input_data(6); // data is not important
-  std::vector<float> alpha_data(6);
-
-  std::vector<float> alpha_scales{0.25f};
-  std::vector<int32_t> zerop{0};
-  Tensor input_tensor = makeInputTensor<DataType::S16>({1, 1, 3, 2}, 0.1, 0, input_data);
-  Tensor alpha_tensor =
-    makeInputTensor<DataType::S16>({1, 1, 1, 2}, alpha_scales, zerop, 1, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.1, 0);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(PreluTest, SInt16_CWQ_uneven_shape1)
-{
-  std::vector<float> input_data{-0.8f, 0.2f, 0.9f, -0.7f, 0.1f, -0.4f};
-  std::vector<float> alpha_data{0.5f, 0.25f};
-  std::vector<float> ref_output_data{-0.4f, 0.2f, 0.9f, -0.175f, 0.1f, -0.1f};
-
-  std::vector<float> alpha_scales{0.05f, 0.025f};
-  std::vector<int32_t> zerop{0, 0};
-  Tensor input_tensor = makeInputTensor<DataType::S16>({1, 1, 3, 2}, 0.1, 0, input_data);
-  Tensor alpha_tensor =
-    makeInputTensor<DataType::S16>({1, 1, 2}, alpha_scales, zerop, 2, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.025, 0);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 1, 3, 2}));
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-TEST(PreluTest, SInt16_CWQ_uneven_shape2)
-{
-  std::vector<float> input_data{
-    0.0f,   0.0f,   0.0f,   // Row 1, Column 1
-    0.5f,   0.5f,   0.5f,   // Row 1, Column 2
-    -1.0f,  -1.0f,  -1.0f,  // Row 2, Column 1
-    -0.25f, -0.25f, -0.25f, // Row 2, Column 2
-  };
-  std::vector<float> alpha_data{0.0f, 0.5f, -0.5f};
-  std::vector<float> ref_output_data{
-    0.0f, 0.0f,    0.0f,  // Row 1, Column 1
-    0.5f, 0.5f,    0.5f,  // Row 1, Column 2
-    0.0f, -0.5f,   0.5f,  // Row 2, Column 1
-    0.0f, -0.125f, 0.125f // Row 2, Column 2
-  };
-
-  std::vector<float> alpha_scales{1.f, 0.05f, 0.1f};
-  std::vector<int32_t> zerop{0, 0, 0};
-  Tensor input_tensor = makeInputTensor<DataType::S16>({1, 2, 2, 3}, 0.01, 0, input_data);
-  Tensor alpha_tensor =
-    makeInputTensor<DataType::S16>({1, 1, 1, 3}, alpha_scales, zerop, 3, alpha_data);
-  Tensor output_tensor = makeOutputTensor(DataType::S16, 0.001, 0);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray({1, 2, 2, 3}));
-  EXPECT_THAT(dequantizeTensorData(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-TEST(PreluTest, Input_Output_Type_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor alpha_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor output_tensor = makeOutputTensor(DataType::U8);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(PreluTest, Input_Alpha_Type_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor alpha_tensor = makeInputTensor<DataType::U8>({1}, {1});
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(PreluTest, Invalid_Input_Type_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::S64>({1}, {1});
-  Tensor alpha_tensor = makeInputTensor<DataType::S64>({1}, {1});
-  Tensor output_tensor = makeOutputTensor(DataType::S64);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  kernel.configure();
-  EXPECT_ANY_THROW(kernel.execute());
-}
-
-TEST(PreluTest, Input_Output_U8_CWQ_NEG)
-{
-  std::vector<float> scales{1.f, 1.f};
-  std::vector<int32_t> zerop{0, 0};
-  std::vector<float> dummy_data(4, 0.f);
-  Tensor input_tensor = makeInputTensor<DataType::U8>({2, 2}, scales, zerop, 0, dummy_data);
-  Tensor alpha_tensor = makeInputTensor<DataType::U8>({2, 2}, scales, zerop, 0, dummy_data);
-  Tensor output_tensor = makeInputTensor<DataType::U8>({2, 2}, scales, zerop, 0, dummy_data);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(PreluTest, Input_Output_S16_CWQ_NEG)
-{
-  std::vector<float> scales{1.f, 1.f};
-  std::vector<int32_t> zerop{0, 0};
-  std::vector<float> dummy_data(4, 0.f);
-  Tensor input_tensor = makeInputTensor<DataType::S16>({2, 2}, scales, zerop, 0, dummy_data);
-  Tensor alpha_tensor = makeInputTensor<DataType::S16>({2, 2}, scales, zerop, 0, dummy_data);
-  Tensor output_tensor = makeInputTensor<DataType::S16>({2, 2}, scales, zerop, 0, dummy_data);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(PreluTest, Mixing_U8_S16_NEG)
-{
-  std::vector<float> dummy_data(4, 0.f);
-  Tensor input_tensor = makeInputTensor<DataType::U8>({2, 2}, 1.f, 0, dummy_data);
-  Tensor alpha_tensor = makeInputTensor<DataType::S16>({2, 2}, 1.f, 0, dummy_data);
-  Tensor output_tensor = makeInputTensor<DataType::U8>({2, 2}, 1.f, 0, dummy_data);
-
-  Prelu kernel(&input_tensor, &alpha_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Relu6.cpp b/compiler/luci-interpreter/src/kernels/Relu6.cpp
deleted file mode 100644
index fa7aa504a..000000000
--- a/compiler/luci-interpreter/src/kernels/Relu6.cpp
+++ /dev/null
@@ -1,88 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Relu6.h"
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-Relu6::Relu6(const Tensor *input, Tensor *output) : Kernel({input}, {output}) {}
-
-void Relu6::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-
-  if (input()->element_type() == DataType::U8)
-  {
-    double multiplier = input()->scale() / output()->scale();
-    quantizeMultiplier(multiplier, &_output_multiplier, &_output_shift);
-  }
-  output()->resize(input()->shape());
-}
-
-void Relu6::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::U8:
-      evalQuantized();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void Relu6::evalFloat() const
-{
-  const auto input_data = getTensorData<float>(input());
-  const auto input_shape = getTensorShape(input());
-  auto output_data = getTensorData<float>(output());
-  auto output_shape = getTensorShape(output());
-
-  tflite::optimized_ops::Relu6(input_shape, input_data, output_shape, output_data);
-}
-
-void Relu6::evalQuantized() const
-{
-  tflite::ReluParams params;
-  params.input_offset = input()->zero_point();
-  params.output_offset = output()->zero_point();
-  params.output_multiplier = _output_multiplier;
-  params.output_shift = _output_shift;
-
-  params.quantized_activation_min =
-    std::max(static_cast<int32_t>(std::numeric_limits<uint8_t>::min()), params.output_offset);
-  params.quantized_activation_max =
-    std::min(static_cast<int32_t>(std::numeric_limits<uint8_t>::max()),
-             params.output_offset + static_cast<int32>(roundf(6.f / output()->scale())));
-
-  tflite::optimized_ops::ReluX(params, getTensorShape(input()), getTensorData<uint8_t>(input()),
-                               getTensorShape(output()), getTensorData<uint8_t>(output()));
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/ResizeNearestNeighbor.cpp b/compiler/luci-interpreter/src/kernels/ResizeNearestNeighbor.cpp
deleted file mode 100644
index c52264997..000000000
--- a/compiler/luci-interpreter/src/kernels/ResizeNearestNeighbor.cpp
+++ /dev/null
@@ -1,74 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2019 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/ResizeNearestNeighbor.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/reference_ops.h>
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-ResizeNearestNeighbor::ResizeNearestNeighbor(const Tensor *input, const Tensor *size,
-                                             Tensor *output,
-                                             const ResizeNearestNeighborParams &params)
-  : KernelWithParams<ResizeNearestNeighborParams>({input, size}, {output}, params)
-{
-}
-
-void ResizeNearestNeighbor::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->shape().num_dims() == 4);
-  LUCI_INTERPRETER_CHECK(size()->shape().num_dims() == 1);
-  LUCI_INTERPRETER_CHECK(size()->element_type() == DataType::S32);
-  LUCI_INTERPRETER_CHECK(size()->shape().dim(0) == 2);
-  Shape output_shape(4);
-  output_shape.dim(0) = input()->shape().dim(0);
-  output_shape.dim(1) = getTensorData<int32_t>(size())[0];
-  output_shape.dim(2) = getTensorData<int32_t>(size())[1];
-  output_shape.dim(3) = input()->shape().dim(3);
-  output()->resize(output_shape);
-}
-
-void ResizeNearestNeighbor::execute() const
-{
-  tflite::ResizeNearestNeighborParams op_params{};
-  op_params.align_corners = params().align_corners;
-  op_params.half_pixel_centers = params().half_pixel_centers;
-  switch (output()->element_type())
-  {
-    case DataType::FLOAT32:
-      tflite::reference_ops::ResizeNearestNeighbor(
-        op_params, getTensorShape(input()), getTensorData<int32_t>(input()), getTensorShape(size()),
-        getTensorData<int32_t>(size()), getTensorShape(output()), getTensorData<int32_t>(output()));
-      break;
-    case DataType::U8:
-      tflite::optimized_ops::ResizeNearestNeighbor(
-        op_params, getTensorShape(input()), getTensorData<uint8_t>(input()), getTensorShape(size()),
-        getTensorData<int32_t>(size()), getTensorShape(output()), getTensorData<uint8_t>(output()));
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Reverse.h b/compiler/luci-interpreter/src/kernels/Reverse.h
deleted file mode 100644
index 3489dae28..000000000
--- a/compiler/luci-interpreter/src/kernels/Reverse.h
+++ /dev/null
@@ -1,43 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_REVERSE_H
-#define LUCI_INTERPRETER_KERNELS_REVERSE_H
-
-#include "core/Kernel.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Reverse : public Kernel
-{
-public:
-  Reverse(const Tensor *input, const Tensor *axes, Tensor *output);
-
-  const Tensor *input() const { return _inputs[0]; }
-  const Tensor *axes() const { return _inputs[1]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_REVERSE_H
diff --git a/compiler/luci-interpreter/src/kernels/Rsqrt.cpp b/compiler/luci-interpreter/src/kernels/Rsqrt.cpp
deleted file mode 100644
index 6dd92dc98..000000000
--- a/compiler/luci-interpreter/src/kernels/Rsqrt.cpp
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Rsqrt.h"
-#include "kernels/Utils.h"
-
-#include <stdexcept>
-#include <cmath>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-Rsqrt::Rsqrt(const Tensor *input, Tensor *output) : Kernel({input}, {output}) {}
-
-void Rsqrt::configure()
-{
-  if (input()->element_type() != output()->element_type())
-  {
-    throw std::runtime_error("Input/output tensor data type mismatch.");
-  }
-  output()->resize(input()->shape());
-}
-
-void Rsqrt::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void Rsqrt::evalFloat() const
-{
-  auto in = getTensorData<float>(input());
-  auto out = getTensorData<float>(output());
-  auto size = getTensorShape(input()).FlatSize();
-  for (auto i = in; i != in + size; ++i)
-  {
-    *out = 1.f / std::sqrt(*i);
-    ++out;
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Rsqrt.h b/compiler/luci-interpreter/src/kernels/Rsqrt.h
deleted file mode 100644
index adc5bcfa2..000000000
--- a/compiler/luci-interpreter/src/kernels/Rsqrt.h
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_RSQRT_H
-#define LUCI_INTERPRETER_KERNELS_RSQRT_H
-
-#include "core/Kernel.h"
-#include "core/KernelParams.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Rsqrt : public Kernel
-{
-public:
-  Rsqrt(const Tensor *input, Tensor *output);
-
-  const Tensor *input() const { return _inputs[0]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  void evalFloat() const;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_RSQRT_H
diff --git a/compiler/luci-interpreter/src/kernels/Slice.cpp b/compiler/luci-interpreter/src/kernels/Slice.cpp
deleted file mode 100644
index 626521815..000000000
--- a/compiler/luci-interpreter/src/kernels/Slice.cpp
+++ /dev/null
@@ -1,149 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Slice.h"
-#include "Utils.h"
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-#include <cassert>
-#include <cstring>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-const int max_dim = 4;
-
-Slice::Slice(const Tensor *input, const Tensor *begin, const Tensor *size, Tensor *output)
-  : Kernel({input, begin, size}, {output})
-{
-}
-
-template <typename T>
-Shape calculateOutputShape(const Tensor *input, const Tensor *begin, const Tensor *size)
-{
-  Shape output_shape = Shape(input->shape().num_dims());
-  for (int idx = 0; idx < input->shape().num_dims(); idx++)
-  {
-    T size_value = getTensorData<T>(size)[idx];
-    if (size_value < 0)
-    {
-      if (size_value != -1)
-      {
-        throw std::runtime_error("Invalid size.");
-      }
-      size_value = input->shape().dim(idx) - getTensorData<T>(begin)[idx];
-    }
-    else
-    {
-      if (input->shape().dim(idx) < getTensorData<T>(begin)[idx] + size_value)
-      {
-        throw std::runtime_error("Invalid begin and size.");
-      }
-    }
-    output_shape.dim(idx) = static_cast<int>(size_value);
-  }
-  return output_shape;
-}
-
-template <typename T>
-void getBeginAndSizeVectors(int dimensions, const Tensor *begin, const Tensor *size,
-                            std::vector<int> *begins, std::vector<int> *sizes)
-{
-  for (int idx = dimensions - 1; idx >= 0; --idx)
-  {
-    begins->push_back(getTensorData<T>(begin)[idx]);
-    sizes->push_back(getTensorData<T>(size)[idx]);
-  }
-}
-
-void Slice::configure()
-{
-  assert(input()->element_type() == output()->element_type());
-  assert(begin()->element_type() == DataType::S32 || begin()->element_type() == DataType::S64);
-  assert(size()->element_type() == DataType::S32 || size()->element_type() == DataType::S64);
-  assert(begin()->shape().num_dims() == 1);
-  assert(size()->shape().num_dims() == 1);
-  assert(input()->shape().num_dims() <= max_dim);
-
-  if (begin()->element_type() == DataType::S32)
-  {
-    output()->resize(calculateOutputShape<int32_t>(input(), begin(), size()));
-  }
-  else if (begin()->element_type() == DataType::S64)
-  {
-    output()->resize(calculateOutputShape<int64_t>(input(), begin(), size()));
-  }
-  else
-  {
-    throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void Slice::execute() const
-{
-  std::vector<int> begins;
-  begins.reserve(max_dim);
-  std::vector<int> sizes;
-  sizes.reserve(max_dim);
-  if (begin()->element_type() == DataType::S32)
-  {
-    getBeginAndSizeVectors<int32_t>(input()->shape().num_dims(), begin(), size(), &begins, &sizes);
-  }
-  else if (begin()->element_type() == DataType::S64)
-  {
-    getBeginAndSizeVectors<int64_t>(input()->shape().num_dims(), begin(), size(), &begins, &sizes);
-  }
-  else
-  {
-    throw std::runtime_error("Unsupported begin type.");
-  }
-  for (int i = input()->shape().num_dims(); i < max_dim; ++i)
-  {
-    begins.push_back(0);
-    sizes.push_back(1);
-  }
-
-  assert(begins.size() == 4);
-  assert(sizes.size() == 4);
-  tflite::SliceParams op_params{};
-  op_params.begin_count = 4;
-  op_params.size_count = 4;
-  for (int i = 0; i < 4; i++)
-  {
-    op_params.begin[i] = begins[3 - i];
-    op_params.size[i] = sizes[3 - i];
-  }
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      tflite::optimized_ops::Slice(op_params, getTensorShape(input()),
-                                   getTensorData<float>(input()), getTensorShape(output()),
-                                   getTensorData<float>(output()));
-      break;
-    case DataType::U8:
-      tflite::optimized_ops::Slice(op_params, getTensorShape(input()),
-                                   getTensorData<uint8_t>(input()), getTensorShape(output()),
-                                   getTensorData<uint8_t>(output()));
-      break;
-    default:
-      throw std::runtime_error("Unsupported input type.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Slice.h b/compiler/luci-interpreter/src/kernels/Slice.h
deleted file mode 100644
index 23c359608..000000000
--- a/compiler/luci-interpreter/src/kernels/Slice.h
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_SLICE_H
-#define LUCI_INTERPRETER_KERNELS_SLICE_H
-
-#include "core/Kernel.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Slice : public Kernel
-{
-public:
-  Slice(const Tensor *input, const Tensor *begin, const Tensor *size, Tensor *output);
-
-  const Tensor *input() const { return _inputs[0]; }
-  const Tensor *begin() const { return _inputs[1]; }
-  const Tensor *size() const { return _inputs[2]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_SLICE_H
diff --git a/compiler/luci-interpreter/src/kernels/Softmax.cpp b/compiler/luci-interpreter/src/kernels/Softmax.cpp
deleted file mode 100644
index 8e29f53ee..000000000
--- a/compiler/luci-interpreter/src/kernels/Softmax.cpp
+++ /dev/null
@@ -1,90 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Softmax.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/softmax.h>
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-Softmax::Softmax(const Tensor *input, Tensor *output, const SoftmaxParams &params)
-  : KernelWithParams<SoftmaxParams>({input}, {output}, params)
-{
-}
-
-void Softmax::configure()
-{
-  LUCI_INTERPRETER_CHECK(input()->element_type() == output()->element_type());
-  LUCI_INTERPRETER_CHECK(input()->shape().num_dims() >= 1);
-  if (input()->element_type() == DataType::U8 || input()->element_type() == DataType::S8)
-  {
-    LUCI_INTERPRETER_CHECK(output()->zero_point() == 0);
-    tflite::SoftmaxParams op_params{};
-    op_params.table = _table;
-    tflite::optimized_ops::PopulateSoftmaxLookupTable(&op_params, input()->scale(), params().beta);
-  }
-  output()->resize(input()->shape());
-}
-
-void Softmax::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      evalFloat();
-      break;
-    case DataType::S8:
-      evalQuantized<int8_t>();
-      break;
-    case DataType::U8:
-      evalQuantized<uint8_t>();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-void Softmax::evalFloat() const
-{
-  tflite::SoftmaxParams op_params{};
-  op_params.beta = params().beta;
-
-  tflite::reference_ops::Softmax(op_params, getTensorShape(input()), getTensorData<float>(input()),
-                                 getTensorShape(output()), getTensorData<float>(output()));
-}
-
-template <typename T> void Softmax::evalQuantized() const
-{
-  tflite::SoftmaxParams op_params{};
-  op_params.table = const_cast<float *>(_table);
-  op_params.zero_point = output()->zero_point();
-  op_params.scale = output()->scale();
-
-  tflite::optimized_ops::Softmax(op_params, getTensorShape(input()), getTensorData<T>(input()),
-                                 getTensorShape(output()), getTensorData<T>(output()));
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/SpaceToDepth.cpp b/compiler/luci-interpreter/src/kernels/SpaceToDepth.cpp
deleted file mode 100644
index fc999372a..000000000
--- a/compiler/luci-interpreter/src/kernels/SpaceToDepth.cpp
+++ /dev/null
@@ -1,79 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "SpaceToDepth.h"
-#include "Utils.h"
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-SpaceToDepth::SpaceToDepth(const Tensor *input, Tensor *output, const SpaceToDepthParams &params)
-  : KernelWithParams<SpaceToDepthParams>({input}, {output}, params)
-{
-}
-
-void SpaceToDepth::configure()
-{
-  assert(input()->shape().num_dims() == 4);
-  assert(output()->element_type() == DataType::FLOAT32 ||
-         output()->element_type() == DataType::U8 || output()->element_type() == DataType::S8 ||
-         output()->element_type() == DataType::S32 || output()->element_type() == DataType::S64);
-  assert(input()->element_type() == output()->element_type());
-
-  const int block_size = params().block_size;
-  const int32_t input_height = input()->shape().dim(1);
-  const int32_t input_width = input()->shape().dim(2);
-  int32_t output_height = input_height / block_size;
-  int32_t output_width = input_width / block_size;
-
-  assert(input_height == output_height * block_size);
-  assert(input_width == output_width * block_size);
-
-  Shape output_shape(4);
-  output_shape.dim(0) = input()->shape().dim(0);
-  output_shape.dim(1) = output_height;
-  output_shape.dim(2) = output_width;
-  output_shape.dim(3) = input()->shape().dim(3) * block_size * block_size;
-
-  output()->resize(output_shape);
-}
-
-void SpaceToDepth::execute() const
-{
-  tflite::SpaceToDepthParams op_params{};
-  op_params.block_size = params().block_size;
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      tflite::optimized_ops::SpaceToDepth(op_params, getTensorShape(input()),
-                                          getTensorData<float>(input()), getTensorShape(output()),
-                                          getTensorData<float>(output()));
-      break;
-    case DataType::U8:
-      tflite::optimized_ops::SpaceToDepth(op_params, getTensorShape(input()),
-                                          getTensorData<uint8_t>(input()), getTensorShape(output()),
-                                          getTensorData<uint8_t>(output()));
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Split.cpp b/compiler/luci-interpreter/src/kernels/Split.cpp
deleted file mode 100644
index 0da0f3779..000000000
--- a/compiler/luci-interpreter/src/kernels/Split.cpp
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "Split.h"
-
-#include "Utils.h"
-
-#include <tensorflow/lite/kernels/internal/optimized/optimized_ops.h>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-Split::Split(const Tensor *axis, const Tensor *input, std::vector<Tensor *> outputs)
-  : Kernel({axis, input}, std::move(outputs))
-{
-}
-
-void Split::configure()
-{
-  assert(axis()->shape().num_elements() == 1);
-  _axis_value = getTensorData<int32_t>(axis())[0];
-  if (_axis_value < 0)
-    _axis_value += input()->shape().num_dims();
-  assert(_axis_value >= 0 && _axis_value < input()->shape().num_dims());
-
-  const int32_t input_size = input()->shape().dim(_axis_value);
-  assert(input_size % _outputs.size() == 0);
-  const int32_t slice_size = input_size / _outputs.size();
-
-  Shape output_shape = input()->shape();
-  output_shape.dim(_axis_value) = slice_size;
-  for (Tensor *output : _outputs)
-  {
-    output->resize(output_shape);
-  }
-}
-
-void Split::execute() const
-{
-  tflite::SplitParams params{};
-  params.num_split = _outputs.size();
-  params.axis = _axis_value;
-
-#define TF_LITE_SPLIT(scalar)                                                                     \
-  {                                                                                               \
-    VectorOfTensors<scalar, false> all_outputs(_outputs);                                         \
-    tflite::optimized_ops::Split(params, getTensorShape(input()), getTensorData<scalar>(input()), \
-                                 all_outputs.shapes(), all_outputs.data());                       \
-  }
-
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      TF_LITE_SPLIT(float);
-      break;
-    case DataType::U8:
-      TF_LITE_SPLIT(uint8_t);
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-#undef TF_LITE_SPLIT
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Sqrt.test.cpp b/compiler/luci-interpreter/src/kernels/Sqrt.test.cpp
deleted file mode 100644
index e40a91e97..000000000
--- a/compiler/luci-interpreter/src/kernels/Sqrt.test.cpp
+++ /dev/null
@@ -1,80 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Sqrt.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-void Check(std::initializer_list<int32_t> input_shape, std::initializer_list<int32_t> output_shape,
-           std::initializer_list<float> input_data, std::initializer_list<float> output_data)
-{
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>(input_shape, input_data);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  Sqrt kernel(&input_tensor, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  EXPECT_THAT(extractTensorData<float>(output_tensor), FloatArrayNear(output_data));
-  EXPECT_THAT(extractTensorShape(output_tensor), ::testing::ElementsAreArray(output_shape));
-}
-
-TEST(SqrtTest, SimpleSqrt)
-{
-  Check(
-    /*input_shape=*/{1, 2, 4, 1}, /*output_shape=*/{1, 2, 4, 1},
-    /*input_data=*/
-    {
-      0, 8, 2, 4,    //
-      3, 7, 10, 0.3, //
-    },
-    /*output_data=*/
-    {
-      0.0, 2.8284271, 1.4142136, 2,                //
-      1.7320508, 2.6457513, 3.1622777, 0.54772256, //
-    });
-}
-
-TEST(SqrtTest, Input_Output_Type_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::FLOAT32>({1}, {1.f});
-  Tensor output_tensor = makeOutputTensor(DataType::S32);
-
-  Sqrt kernel(&input_tensor, &output_tensor);
-  EXPECT_ANY_THROW(kernel.configure());
-}
-
-TEST(AddTest, Invalid_Input_Type_NEG)
-{
-  Tensor input_tensor = makeInputTensor<DataType::S64>({1}, {1});
-  Tensor output_tensor = makeOutputTensor(DataType::S64);
-
-  Sqrt kernel(&input_tensor, &output_tensor);
-  kernel.configure();
-  EXPECT_ANY_THROW(kernel.execute());
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/SquaredDifference.test.cpp b/compiler/luci-interpreter/src/kernels/SquaredDifference.test.cpp
deleted file mode 100644
index a72eaadfa..000000000
--- a/compiler/luci-interpreter/src/kernels/SquaredDifference.test.cpp
+++ /dev/null
@@ -1,67 +0,0 @@
-/*
- * Copyright (c) 2021 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/SquaredDifference.h"
-#include "kernels/TestUtils.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace
-{
-
-using namespace testing;
-
-TEST(SquaredDifferenceTest, Float)
-{
-  Shape input_shape{3, 1, 2};
-  std::vector<float> input_data1{1.0, 0.0, -1.0, 11.0, -2.0, -1.44};
-  std::vector<float> input_data2{-1.0, 0.0, 1.0, 12.0, -3.0, -1.43};
-  Tensor input_tensor1 = makeInputTensor<DataType::FLOAT32>(input_shape, input_data1);
-  Tensor input_tensor2 = makeInputTensor<DataType::FLOAT32>(input_shape, input_data2);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  SquaredDifference kernel(&input_tensor1, &input_tensor2, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  std::vector<float> ref_output_data{4.0, 0.0, 4.0, 1.0, 1.0, 0.0001};
-  EXPECT_THAT(extractTensorData<float>(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-TEST(SquaredDifferenceTest, FloatBroadcast)
-{
-  Shape input_shape1{3, 1, 2};
-  Shape input_shape2{1};
-  std::vector<float> input_data1{1.0, 0.0, -1.0, 11.0, -2.0, -1.44};
-  std::vector<float> input_data2{1.0};
-  Tensor input_tensor1 = makeInputTensor<DataType::FLOAT32>(input_shape1, input_data1);
-  Tensor input_tensor2 = makeInputTensor<DataType::FLOAT32>(input_shape2, input_data2);
-  Tensor output_tensor = makeOutputTensor(DataType::FLOAT32);
-
-  SquaredDifference kernel(&input_tensor1, &input_tensor2, &output_tensor);
-  kernel.configure();
-  kernel.execute();
-
-  std::vector<float> ref_output_data{0.0, 1.0, 4.0, 100.0, 9.0, 5.9536};
-  EXPECT_THAT(extractTensorData<float>(output_tensor), FloatArrayNear(ref_output_data));
-}
-
-} // namespace
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Squeeze.h b/compiler/luci-interpreter/src/kernels/Squeeze.h
deleted file mode 100644
index 687af5158..000000000
--- a/compiler/luci-interpreter/src/kernels/Squeeze.h
+++ /dev/null
@@ -1,44 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2018 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_SQUEEZE_H
-#define LUCI_INTERPRETER_KERNELS_SQUEEZE_H
-
-#include "core/Kernel.h"
-#include "core/KernelParams.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Squeeze : public KernelWithParams<SqueezeParams>
-{
-public:
-  Squeeze(const Tensor *input, Tensor *output, const SqueezeParams &params);
-
-  const Tensor *input() const { return _inputs[0]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_SQUEEZE_H
diff --git a/compiler/luci-interpreter/src/kernels/StridedSlice.cpp b/compiler/luci-interpreter/src/kernels/StridedSlice.cpp
deleted file mode 100644
index 37b0dd8c5..000000000
--- a/compiler/luci-interpreter/src/kernels/StridedSlice.cpp
+++ /dev/null
@@ -1,145 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/StridedSlice.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/reference_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-StridedSlice::StridedSlice(const Tensor *input, const Tensor *begin, const Tensor *end,
-                           const Tensor *strides, Tensor *output, const StridedSliceParams &params)
-  : KernelWithParams<StridedSliceParams>({input, begin, end, strides}, {output}, params)
-{
-}
-
-void StridedSlice::configure()
-{
-  assert(begin()->shape().num_dims() == 1);
-  assert(end()->shape().num_dims() == 1);
-  assert(strides()->shape().num_dims() == 1);
-  assert(input()->element_type() == output()->element_type());
-  assert(begin()->element_type() == DataType::S32);
-  assert(end()->element_type() == DataType::S32);
-  assert(strides()->element_type() == DataType::S32);
-  assert(input()->shape().num_dims() <= 4);
-  if (params().ellipsis_mask != 0)
-  {
-    throw std::runtime_error("ellipsis_mask is not implemented yet.");
-  }
-  if (params().new_axis_mask != 0)
-  {
-    throw std::runtime_error("new_axis_mask is not implemented yet.");
-  }
-  if (input()->element_type() == DataType::U8)
-  {
-    assert(input()->scale() == output()->scale());
-    assert(input()->zero_point() == output()->zero_point());
-  }
-  tflite::StridedSliceParams op_params{};
-  op_params.start_indices_count = input()->shape().num_dims();
-  op_params.stop_indices_count = input()->shape().num_dims();
-  op_params.strides_count = input()->shape().num_dims();
-
-  for (int i = 0; i < input()->shape().num_dims(); i++)
-  {
-    op_params.start_indices[i] = getTensorData<int32_t>(begin())[i];
-    op_params.stop_indices[i] = getTensorData<int32_t>(end())[i];
-    op_params.strides[i] = getTensorData<int32_t>(strides())[i];
-  }
-  op_params.begin_mask = params().begin_mask;
-  op_params.ellipsis_mask = 0;
-  op_params.end_mask = params().end_mask;
-  op_params.new_axis_mask = 0;
-  op_params.shrink_axis_mask = params().shrink_axis_mask;
-  std::vector<int32_t> output_shape_vector;
-  for (int i = 0; i < input()->shape().num_dims(); i++)
-  {
-    int idx = input()->shape().num_dims() - i - 1;
-    int32_t stride = getTensorData<int32_t>(strides())[idx];
-    assert(stride != 0);
-    int32_t begin = ::tflite::strided_slice::StartForAxis(op_params, getTensorShape(input()), idx);
-    int32_t end =
-      ::tflite::strided_slice::StopForAxis(op_params, getTensorShape(input()), idx, begin);
-
-    const bool shrink_axis = params().shrink_axis_mask & (1 << idx);
-    if (shrink_axis)
-    {
-      end = begin + 1;
-    }
-
-    int32_t dim_shape = std::ceil((end - begin) / static_cast<float>(stride));
-    dim_shape = dim_shape < 0 ? 0 : dim_shape;
-    if (!shrink_axis)
-    {
-      output_shape_vector.push_back(dim_shape);
-    }
-  }
-  Shape output_shape = Shape(output_shape_vector.size());
-  for (size_t i = 0; i < output_shape_vector.size(); i++)
-  {
-    output_shape.dim(i) = output_shape_vector[output_shape_vector.size() - i - 1];
-  }
-  output()->resize(output_shape);
-}
-
-void StridedSlice::execute() const
-{
-  tflite::StridedSliceParams op_params{};
-  op_params.start_indices_count = input()->shape().num_dims();
-  op_params.stop_indices_count = input()->shape().num_dims();
-  op_params.strides_count = input()->shape().num_dims();
-
-  for (int i = 0; i < input()->shape().num_dims(); i++)
-  {
-    op_params.start_indices[i] = getTensorData<int32_t>(begin())[i];
-    op_params.stop_indices[i] = getTensorData<int32_t>(end())[i];
-    op_params.strides[i] = getTensorData<int32_t>(strides())[i];
-  }
-  op_params.begin_mask = params().begin_mask;
-  op_params.ellipsis_mask = 0;
-  op_params.end_mask = params().end_mask;
-  op_params.new_axis_mask = 0;
-  op_params.shrink_axis_mask = params().shrink_axis_mask;
-
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      tflite::reference_ops::StridedSlice(op_params, getTensorShape(input()),
-                                          getTensorData<float>(input()), getTensorShape(output()),
-                                          getTensorData<float>(output()));
-      break;
-    case DataType::U8:
-      tflite::reference_ops::StridedSlice(op_params, getTensorShape(input()),
-                                          getTensorData<uint8_t>(input()), getTensorShape(output()),
-                                          getTensorData<uint8_t>(output()));
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Tanh.h b/compiler/luci-interpreter/src/kernels/Tanh.h
deleted file mode 100644
index 8017c9638..000000000
--- a/compiler/luci-interpreter/src/kernels/Tanh.h
+++ /dev/null
@@ -1,52 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_TANH_H
-#define LUCI_INTERPRETER_KERNELS_TANH_H
-
-#include "core/Kernel.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Tanh : public Kernel
-{
-public:
-  Tanh(const Tensor *input, Tensor *output);
-
-  const Tensor *input() const { return _inputs[0]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  void evalFloat() const;
-  void evalQuantized() const;
-  void populateLookupTable();
-  void setTableValue(uint8_t value, uint8_t idx) { _table[idx] = value; };
-  uint8_t getTableValue(uint8_t idx) const { return _table[idx]; };
-
-private:
-  uint8_t _table[256]{};
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_TANH_H
diff --git a/compiler/luci-interpreter/src/kernels/TestUtils.cpp b/compiler/luci-interpreter/src/kernels/TestUtils.cpp
deleted file mode 100644
index 831dc4247..000000000
--- a/compiler/luci-interpreter/src/kernels/TestUtils.cpp
+++ /dev/null
@@ -1,123 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/TestUtils.h"
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-namespace testing
-{
-
-using ::testing::FloatNear;
-using ::testing::Matcher;
-
-Tensor makeOutputTensor(DataType element_type) { return Tensor(element_type, {}, {}, ""); }
-
-Tensor makeOutputTensor(DataType element_type, float scale, int32_t zero_point)
-{
-  return Tensor(element_type, {}, {{scale}, {zero_point}}, "");
-}
-
-std::vector<float> dequantizeTensorData(const Tensor &tensor)
-{
-  if (tensor.element_type() == DataType::U8)
-  {
-    std::vector<uint8_t> data = extractTensorData<uint8_t>(tensor);
-    return dequantize(data.data(), data.size(), tensor.scale(), tensor.zero_point());
-  }
-  else if (tensor.element_type() == DataType::S16)
-  {
-    // S16 quantization is symmetric, so zero point should be zero.
-    for (auto zp : tensor.zero_points())
-    {
-      (void)zp;
-      assert(zp == 0);
-    }
-
-    std::vector<int16_t> data = extractTensorData<int16_t>(tensor);
-    if (tensor.scales().size() == 1)
-    {
-      return dequantize(data.data(), data.size(), tensor.scale(), 0);
-    }
-
-    // quantize_dimension breaks shape into two parts:
-    // inner dimensions that contains continuous data with one quantization type
-    // outer dimensions that contains other dimensions
-    const Shape shape = tensor.shape();
-    const int32_t quantized_dimension = tensor.quantized_dimension();
-    assert(quantized_dimension < shape.num_dims());
-    size_t outer_dims_size = 1;
-    int32_t quant_dim_size = shape.dim(quantized_dimension);
-    size_t inner_dims_size = 1;
-    assert(quant_dim_size == tensor.scales().size());
-
-    for (int i = 0; i < quantized_dimension; ++i)
-      outer_dims_size *= shape.dim(i);
-    for (int i = quantized_dimension + 1; i < shape.num_dims(); ++i)
-      inner_dims_size *= shape.dim(i);
-
-    assert(shape.num_elements() == outer_dims_size * quant_dim_size * inner_dims_size);
-
-    std::vector<float> dequantized_data;
-    dequantized_data.reserve(shape.num_elements());
-    for (size_t outer_it = 0; outer_it < outer_dims_size; ++outer_it)
-      for (int32_t channel = 0; channel < quant_dim_size; ++channel)
-      {
-        float scale = tensor.scales()[channel];
-        size_t offset = inner_dims_size * (quant_dim_size * outer_it + channel);
-        std::vector<float> part_dequantized_data =
-          dequantize(data.data() + offset, inner_dims_size, scale, 0);
-        dequantized_data.insert(dequantized_data.end(), part_dequantized_data.begin(),
-                                part_dequantized_data.end());
-      }
-    return dequantized_data;
-  }
-  else
-  {
-    throw std::runtime_error("Unsupported type.");
-  }
-}
-
-Matcher<std::vector<float>> FloatArrayNear(const std::vector<float> &values, float max_abs_error)
-{
-  std::vector<Matcher<float>> matchers;
-  matchers.reserve(values.size());
-  for (const float v : values)
-  {
-    matchers.emplace_back(FloatNear(v, max_abs_error));
-  }
-  return ElementsAreArray(matchers);
-}
-
-std::vector<int32_t> extractTensorShape(const Tensor &tensor)
-{
-  std::vector<int32_t> result;
-  int dims = tensor.shape().num_dims();
-  for (int i = 0; i < dims; i++)
-  {
-    result.push_back(tensor.shape().dim(i));
-  }
-  return result;
-}
-
-} // namespace testing
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/TransposeConv.h b/compiler/luci-interpreter/src/kernels/TransposeConv.h
deleted file mode 100644
index 2e0beece8..000000000
--- a/compiler/luci-interpreter/src/kernels/TransposeConv.h
+++ /dev/null
@@ -1,66 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_TRANSPOSECONV_H
-#define LUCI_INTERPRETER_KERNELS_TRANSPOSECONV_H
-
-#include "core/Kernel.h"
-#include "core/KernelParams.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class ChannelQuantMultipliers;
-
-class TransposeConv : public KernelWithParams<TransposeConvParams>
-{
-public:
-  TransposeConv(const Tensor *output_shape, const Tensor *filter, const Tensor *input,
-                const Tensor *bias, Tensor *output, const TransposeConvParams &params);
-
-  ~TransposeConv();
-
-  const Tensor *output_shape() const { return _inputs[0]; }
-  const Tensor *filter() const { return _inputs[1]; }
-  const Tensor *input() const { return _inputs[2]; }
-  const Tensor *bias() const { return _inputs[3]; }
-  Tensor *output() const { return _outputs[0]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  void evalFloat() const;
-  void evalQuantized() const;
-  void evalQuantizedPerChannel() const;
-  void evalQuantizedS16() const;
-
-private:
-  std::unique_ptr<Tensor> _scratch_tensor;
-
-  int32_t _padding_height{};
-  int32_t _padding_width{};
-  // The scaling factor from input to output (aka the 'real multiplier') can
-  // be represented as a fixed point multiplier plus a left shift.
-  std::vector<ChannelQuantMultipliers> _quant_multipliers;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_TRANSPOSECONV_H
diff --git a/compiler/luci-interpreter/src/kernels/Unpack.cpp b/compiler/luci-interpreter/src/kernels/Unpack.cpp
deleted file mode 100644
index 9127241c0..000000000
--- a/compiler/luci-interpreter/src/kernels/Unpack.cpp
+++ /dev/null
@@ -1,84 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Unpack.h"
-
-#include "kernels/Utils.h"
-
-#include <tensorflow/lite/kernels/internal/reference/reference_ops.h>
-
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-
-namespace kernels
-{
-
-Unpack::Unpack(const Tensor *input, std::vector<Tensor *> outputs, const UnpackParams &params)
-  : KernelWithParams<UnpackParams>({input}, std::move(outputs), params)
-{
-}
-
-void Unpack::configure()
-{
-  const Shape &input_shape = input()->shape();
-
-  int axis = _params.axis;
-  if (axis < 0)
-    axis += input()->shape().num_dims();
-  assert(axis >= 0 && axis < input_shape.num_dims());
-
-  Shape output_shape(input_shape.num_dims() - 1);
-  int out_index = 0;
-  for (int in_index = 0; in_index < input_shape.num_dims(); ++in_index)
-  {
-    if (in_index != axis)
-      output_shape.dim(out_index++) = input_shape.dim(in_index);
-  }
-
-  for (Tensor *output : _outputs)
-  {
-    assert(output->element_type() == input()->element_type());
-    output->resize(output_shape);
-  }
-}
-
-template <typename T> void Unpack::executeImpl() const
-{
-  tflite::UnpackParams params{};
-  params.axis = _params.axis;
-  params.num_split = _outputs.size();
-  VectorOfTensors<T, false> all_outputs(_outputs);
-  tflite::reference_ops::Unpack<T>(params, getTensorShape(input()), getTensorData<T>(input()),
-                                   **all_outputs.shapes(), all_outputs.data());
-}
-
-void Unpack::execute() const
-{
-  switch (input()->element_type())
-  {
-    case DataType::FLOAT32:
-      return executeImpl<float>();
-    case DataType::U8:
-      return executeImpl<uint8_t>();
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci-interpreter/src/kernels/Unpack.h b/compiler/luci-interpreter/src/kernels/Unpack.h
deleted file mode 100644
index f4a44ecad..000000000
--- a/compiler/luci-interpreter/src/kernels/Unpack.h
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#ifndef LUCI_INTERPRETER_KERNELS_UNPACK_H
-#define LUCI_INTERPRETER_KERNELS_UNPACK_H
-
-#include "core/Kernel.h"
-#include "core/KernelParams.h"
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-class Unpack : public KernelWithParams<UnpackParams>
-{
-public:
-  Unpack(const Tensor *input, std::vector<Tensor *> outputs, const UnpackParams &params);
-
-  const Tensor *input() const { return _inputs[0]; }
-  Tensor *output(int index) const { return _outputs[index]; }
-
-  void configure() override;
-  void execute() const override;
-
-private:
-  template <typename T> void executeImpl() const;
-};
-
-} // namespace kernels
-} // namespace luci_interpreter
-
-#endif // LUCI_INTERPRETER_KERNELS_UNPACK_H
diff --git a/compiler/luci-interpreter/src/kernels/Utils.cpp b/compiler/luci-interpreter/src/kernels/Utils.cpp
deleted file mode 100644
index 83faa7d7f..000000000
--- a/compiler/luci-interpreter/src/kernels/Utils.cpp
+++ /dev/null
@@ -1,187 +0,0 @@
-/*
- * Copyright (c) 2020 Samsung Electronics Co., Ltd. All Rights Reserved
- * Copyright 2017 The TensorFlow Authors. All Rights Reserved.
- *
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "kernels/Utils.h"
-
-#include <cassert>
-#include <cmath>
-#include <limits>
-#include <stdexcept>
-
-namespace luci_interpreter
-{
-namespace kernels
-{
-
-void calculateActivationRange(Activation activation, float *activation_min, float *activation_max)
-{
-  switch (activation)
-  {
-    case Activation::NONE:
-    case Activation::TANH:
-      *activation_min = std::numeric_limits<float>::lowest();
-      *activation_max = std::numeric_limits<float>::max();
-      break;
-    case Activation::RELU:
-      *activation_min = 0;
-      *activation_max = std::numeric_limits<float>::max();
-      break;
-    case Activation::RELU_N1_TO_1:
-      *activation_min = -1;
-      *activation_max = 1;
-      break;
-    case Activation::RELU6:
-      *activation_min = 0;
-      *activation_max = 6;
-      break;
-    default:
-      throw std::runtime_error("Unsupported activation.");
-  }
-}
-
-static void calculateActivationRangeQuantizedImpl(Activation activation, int32_t qmin, int32_t qmax,
-                                                  const Tensor *output, int32_t *activation_min,
-                                                  int32_t *activation_max)
-{
-  const float scale = output->scale();
-  const int32_t zero_point = output->zero_point();
-
-  auto quantize = [scale, zero_point](float x) {
-    return zero_point + static_cast<int32_t>(std::round(x / scale));
-  };
-
-  switch (activation)
-  {
-    case Activation::NONE:
-    case Activation::TANH:
-      *activation_min = qmin;
-      *activation_max = qmax;
-      break;
-    case Activation::RELU:
-      *activation_min = std::max(qmin, quantize(0.0f));
-      *activation_max = qmax;
-      break;
-    case Activation::RELU_N1_TO_1:
-      *activation_min = std::max(qmin, quantize(-1.0f));
-      *activation_max = std::min(qmax, quantize(1.0f));
-      break;
-    case Activation::RELU6:
-      *activation_min = std::max(qmin, quantize(0.0f));
-      *activation_max = std::min(qmax, quantize(6.0f));
-      break;
-    default:
-      throw std::runtime_error("Unsupported activation.");
-  }
-}
-
-void calculateActivationRangeQuantized(Activation activation, const Tensor *output,
-                                       int32_t *activation_min, int32_t *activation_max)
-{
-  // For now, assume that signed type implies signed symmetric quantization.
-  int32_t qmin{};
-  int32_t qmax{};
-  switch (output->element_type())
-  {
-    case DataType::U8:
-      qmin = 0;
-      qmax = std::numeric_limits<uint8_t>::max();
-      break;
-    case DataType::S8:
-      assert(output->zero_point() == 0);
-      qmin = -std::numeric_limits<int8_t>::max();
-      qmax = std::numeric_limits<int8_t>::max();
-      break;
-    case DataType::S16:
-      assert(output->zero_point() == 0);
-      qmin = -std::numeric_limits<int16_t>::max();
-      qmax = std::numeric_limits<int16_t>::max();
-      break;
-    default:
-      throw std::runtime_error("Unsupported type.");
-  }
-
-  calculateActivationRangeQuantizedImpl(activation, qmin, qmax, output, activation_min,
-                                        activation_max);
-}
-
-void quantizeMultiplier(double double_multiplier, int32_t *quantized_multiplier, int *shift)
-{
-  if (double_multiplier == 0.0)
-  {
-    *quantized_multiplier = 0;
-    *shift = 0;
-    return;
-  }
-
-  const double q = std::frexp(double_multiplier, shift);
-  auto q_fixed = static_cast<int64_t>(std::round(q * (INT64_C(1) << 31)));
-
-  if (q_fixed == (INT64_C(1) << 31))
-  {
-    q_fixed /= 2;
-    ++*shift;
-  }
-  assert(q_fixed <= std::numeric_limits<int32_t>::max());
-  // A shift amount smaller than -31 would cause all bits to be shifted out
-  // and thus all results would be zero. We implement that instead with
-  // q_fixed==0, so as to avoid hitting issues with right-shift
-  // operations with shift amounts greater than 31. Note that this happens
-  // roughly when abs(double_multiplier) < 2^-31 and the present handling means
-  // that we're effectively flushing tiny double_multiplier's to zero.
-  // We could conceivably handle values in the range (roughly) [32, 63]
-  // as 'denormals' i.e. (shift==0, q_fixed < 2^30). In that point of view
-  // the present handling is just doing 'flush denormals to zero'. We could
-  // reconsider and actually generate nonzero denormals if a need arises.
-  if (*shift < -31)
-  {
-    *shift = 0;
-    q_fixed = 0;
-  }
-  *quantized_multiplier = static_cast<int32_t>(q_fixed);
-}
-
-void quantizeMultiplierSmallerThanOneExp(double double_multiplier, int32_t *quantized_multiplier,
-                                         int *left_shift)
-{
-  assert(double_multiplier < 1.0);
-  assert(double_multiplier > 0.0);
-  int shift;
-  quantizeMultiplier(double_multiplier, quantized_multiplier, &shift);
-  assert(shift <= 0);
-  *left_shift = shift;
-}
-
-Shape calculateShapeForBroadcast(const Shape &input1_shape, const Shape &input2_shape)
-{
-  const int num_input1_dims = input1_shape.num_dims();
-  const int num_input2_dims = input2_shape.num_dims();
-  const int num_out_dims = std::max(num_input1_dims, num_input2_dims);
-  Shape output_shape(num_out_dims);
-
-  for (int i = 0; i < num_out_dims; ++i)
-  {
-    const int32_t input1_dim = i < num_input1_dims ? input1_shape.dim(num_input1_dims - i - 1) : 1;
-    const int32_t input2_dim = i < num_input2_dims ? input2_shape.dim(num_input2_dims - i - 1) : 1;
-    assert(input1_dim == input2_dim || input1_dim == 1 || input2_dim == 1);
-    output_shape.dim(num_out_dims - i - 1) = std::max(input1_dim, input2_dim);
-  }
-
-  return output_shape;
-}
-
-} // namespace kernels
-} // namespace luci_interpreter
diff --git a/compiler/luci/lang/CMakeLists.txt b/compiler/luci/lang/CMakeLists.txt
index 669a866b1..1ddee19ee 100644
--- a/compiler/luci/lang/CMakeLists.txt
+++ b/compiler/luci/lang/CMakeLists.txt
@@ -2,7 +2,7 @@ file(GLOB_RECURSE SOURCES "src/*.cpp")
 file(GLOB_RECURSE TESTS "src/*.test.cpp")
 list(REMOVE_ITEM SOURCES ${TESTS})
 
-add_library(luci_lang SHARED ${SOURCES})
+add_library(luci_lang STATIC ${SOURCES})
 target_include_directories(luci_lang PRIVATE src)
 target_include_directories(luci_lang PUBLIC include)
 target_link_libraries(luci_lang PUBLIC loco)
diff --git a/compiler/luci/lang/include/luci/IR/CircleNodeMixins.h b/compiler/luci/lang/include/luci/IR/CircleNodeMixins.h
index 3f8ab7d61..df80fd67b 100644
--- a/compiler/luci/lang/include/luci/IR/CircleNodeMixins.h
+++ b/compiler/luci/lang/include/luci/IR/CircleNodeMixins.h
@@ -82,7 +82,7 @@ public:
   virtual ~FixedArityNode() = default;
 
 public:
-  unsigned arity(void) const final { return N; }
+    uint32_t arity(void) const final { return N; }
 
   loco::Node *arg(uint32_t n) const final { return _args.at(n)->node(); }
 
diff --git a/compiler/luci/log/CMakeLists.txt b/compiler/luci/log/CMakeLists.txt
index 23bd00828..9c6ef8d29 100644
--- a/compiler/luci/log/CMakeLists.txt
+++ b/compiler/luci/log/CMakeLists.txt
@@ -1,7 +1,7 @@
 # TODO Find how to test logging framework
 file(GLOB_RECURSE SOURCES "src/*.cpp")
 
-add_library(luci_log SHARED ${SOURCES})
+add_library(luci_log STATIC ${SOURCES})
 target_include_directories(luci_log PUBLIC include)
 target_link_libraries(luci_log PUBLIC hermes)
 target_link_libraries(luci_log PRIVATE hermes_std)
diff --git a/infra/nncc/CMakeLists.txt b/infra/nncc/CMakeLists.txt
index eb279902e..bb8cfdcf4 100644
--- a/infra/nncc/CMakeLists.txt
+++ b/infra/nncc/CMakeLists.txt
@@ -1,155 +1,155 @@
-cmake_minimum_required(VERSION 3.1)
-
-project(nncc)
-
-enable_testing()
-
-set(CMAKE_CXX_STANDARD 14)
-
-set(CMAKE_SKIP_BUILD_RPATH FALSE)
-set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)
-set(CMAKE_INSTALL_RPATH "$ORIGIN/../lib:$ORIGIN/")
-set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
-
-# This feature works with CMake 3.5.2 or later. However, using previous versions does not produce
-# an error. We are still officially using CMake 3.1.0, but put this code for the sake of semantic
-# support in various development tools.
-# Todo: Someday, CMake needs to be updated to 3.7.2 or later to take advantage of improvements
-#       such as `cmake-server`.
-set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
-
-set(NNAS_PROJECT_SOURCE_DIR "${CMAKE_CURRENT_LIST_DIR}/../.." CACHE
-  INTERNAL "Where to find nnas top-level source directory"
-)
-
-set(NNAS_EXTERNALS_DIR
-  "${NNAS_PROJECT_SOURCE_DIR}/externals" CACHE
-  INTERNAL "Where to download external dependencies"
-)
-set(NNCC_OVERLAY_DIR "${CMAKE_BINARY_DIR}/overlay" CACHE
-    INTERNAL "Where locally built external dependencies are installed")
-
-# Share package build script with runtime
-set(EXT_OVERLAY_DIR ${NNCC_OVERLAY_DIR})
-
-# This allows find_package to access configurations installed inside overlay
-list(APPEND CMAKE_PREFIX_PATH "${EXT_OVERLAY_DIR}")
-
-macro(nnas_include PREFIX)
-  include("${NNAS_PROJECT_SOURCE_DIR}/infra/cmake/modules/${PREFIX}.cmake")
-endmacro(nnas_include)
-
-macro(nnas_find_package PREFIX)
-  find_package(${PREFIX} CONFIG NO_DEFAULT_PATH
-    PATHS ${NNAS_PROJECT_SOURCE_DIR}/infra/cmake/packages
-    ${ARGN}
-  )
-endmacro(nnas_find_package)
-
-# nncc_find_resource(NAME) will update the following variables
-#
-#   NAME_FOUND
-#   NAME_DIR
-#
-# TODO Explain how to add a resource in README.md
-function(nncc_find_resource NAME)
-  set(RESOURCE_DIR "${NNAS_PROJECT_SOURCE_DIR}/res/${NAME}")
-
-  if(NOT IS_DIRECTORY ${RESOURCE_DIR})
-    set(${NAME}_FOUND FALSE PARENT_SCOPE)
-    return()
-  endif(NOT IS_DIRECTORY ${RESOURCE_DIR})
-
-  set(${NAME}_DIR ${RESOURCE_DIR} PARENT_SCOPE)
-  set(${NAME}_FOUND TRUE PARENT_SCOPE)
-endfunction(nncc_find_resource)
-
-###
-### CMake configuration
-###
-if(NOT CMAKE_BUILD_TYPE)
-  set(CMAKE_BUILD_TYPE "Debug" CACHE STRING "Type of build" FORCE)
-endif(NOT CMAKE_BUILD_TYPE)
-message(STATUS "Use '${CMAKE_BUILD_TYPE}' configuration")
-
-# Prefer -pthread to -lpthread for find_package(Threads ...)
-#
-# std::thread code compiled only with -lpthread emits the following runtime error (on GCC 4.8.4)
-#
-#   terminate called after throwing an instance of 'std::system_error'
-#     what():  Enable multithreading to use std::thread: Operation not permitted
-#
-set(THREADS_PREFER_PTHREAD_FLAG TRUE)
-
-###
-### Configuration
-###
-option(DOWNLOAD_PROTOBUF "Download Protocol Buffer source" ON)
-option(BUILD_PROTOBUF "Locally build Protocol Buffer from the downloaded source" ON)
-option(DOWNLOAD_EIGEN "Download Eigen source" ON)
-option(DOWNLOAD_FARMHASH "Download farmhash source" ON)
-option(DOWNLOAD_GEMMLOWP "Download GEMM low precesion library source" ON)
-option(DOWNLOAD_RUY "Download ruy source" ON)
-option(DOWNLOAD_NEON2SSE "Download NEON2SSE library source" ON)
-option(DOWNLOAD_GFLAGS "Download GFlags source" OFF)
-option(DOWNLOAD_FLATBUFFERS "Download FlatBuffers source" ON)
-option(BUILD_FLATBUFFERS "Locally build Flatbuffers from the downloaded source" ON)
-option(DOWNLOAD_TENSORFLOW "Download TensorFlow source" ON)
-option(DOWNLOAD_CAFFE "Download Caffe source" ON)
-option(DOWNLOAD_PYTORCH "Download Pytorch source" ON)
-option(DOWNLOAD_ONNX "Download ONNX source" ON)
-option(DOWNLOAD_ABSEIL "Download Abseil-cpp source" ON)
-option(DOWNLOAD_OPENCL_HEADERS "Download OpenCl Header source" ON)
-option(DOWNLOAD_PYBIND11 "Download Pybind11 source" ON)
-
-option(DOWNLOAD_GTEST "Download Google Test source" ON)
-option(BUILD_GTEST "Build Google Test from the downloaded source" ON)
-option(DOWNLOAD_HDF5 "Download HDF5 source" ON)
-option(BUILD_HDF5 "Build HDF5 from the downloaded source" ON)
-
-nnas_find_package(GTest QUIET)
-
-option(ENABLE_TEST "Build Tests using Google Test" ${GTest_FOUND})
-
-if(${ENABLE_TEST} AND NOT ${GTest_FOUND})
-  message(FATAL_ERROR "Google Test is required to enable test")
-endif(${ENABLE_TEST} AND NOT ${GTest_FOUND})
-
-option(ENABLE_COVERAGE "Build for coverage test" OFF)
-if(${ENABLE_COVERAGE} AND NOT ${ENABLE_TEST})
-  message(FATAL_ERROR "Test should be enabled to measure test coverage")
-endif(${ENABLE_COVERAGE} AND NOT ${ENABLE_TEST})
-
-if(${ENABLE_TEST})
-  include(CTest)
-endif(${ENABLE_TEST})
-
-option(ENABLE_STRICT_BUILD "Treat warning as error" OFF)
-
-# This option might be turned ON for Windows native build.
-# Check our ProtobufConfig.cmake for its usage.
-option(USE_PROTOBUF_LEGACY_IMPORT "Use legacy MODULE mode import rather than CONFIG mode" OFF)
-
-###
-### Target
-###
+#cmake_minimum_required(VERSION 3.1)
+#
+#project(nncc)
+#
+#enable_testing()
+#
+#set(CMAKE_CXX_STANDARD 14)
+#
+#set(CMAKE_SKIP_BUILD_RPATH FALSE)
+#set(CMAKE_BUILD_WITH_INSTALL_RPATH FALSE)
+#set(CMAKE_INSTALL_RPATH "$ORIGIN/../lib:$ORIGIN/")
+#set(CMAKE_INSTALL_RPATH_USE_LINK_PATH TRUE)
+#
+## This feature works with CMake 3.5.2 or later. However, using previous versions does not produce
+## an error. We are still officially using CMake 3.1.0, but put this code for the sake of semantic
+## support in various development tools.
+## Todo: Someday, CMake needs to be updated to 3.7.2 or later to take advantage of improvements
+##       such as `cmake-server`.
+#set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
+#
+#set(NNAS_PROJECT_SOURCE_DIR "${CMAKE_CURRENT_LIST_DIR}/../.." CACHE
+#  INTERNAL "Where to find nnas top-level source directory"
+#)
+#
+#set(NNAS_EXTERNALS_DIR
+#  "${NNAS_PROJECT_SOURCE_DIR}/externals" CACHE
+#  INTERNAL "Where to download external dependencies"
+#)
+#set(NNCC_OVERLAY_DIR "${CMAKE_BINARY_DIR}/overlay" CACHE
+#    INTERNAL "Where locally built external dependencies are installed")
+#
+## Share package build script with runtime
+#set(EXT_OVERLAY_DIR ${NNCC_OVERLAY_DIR})
+#
+## This allows find_package to access configurations installed inside overlay
+#list(APPEND CMAKE_PREFIX_PATH "${EXT_OVERLAY_DIR}")
+#
+#macro(nnas_include PREFIX)
+#  include("${NNAS_PROJECT_SOURCE_DIR}/infra/cmake/modules/${PREFIX}.cmake")
+#endmacro(nnas_include)
+#
+#macro(nnas_find_package PREFIX)
+#  find_package(${PREFIX} CONFIG NO_DEFAULT_PATH
+#    PATHS ${NNAS_PROJECT_SOURCE_DIR}/infra/cmake/packages
+#    ${ARGN}
+#  )
+#endmacro(nnas_find_package)
+#
+## nncc_find_resource(NAME) will update the following variables
+##
+##   NAME_FOUND
+##   NAME_DIR
+##
+## TODO Explain how to add a resource in README.md
+#function(nncc_find_resource NAME)
+#  set(RESOURCE_DIR "${NNAS_PROJECT_SOURCE_DIR}/res/${NAME}")
+#
+#  if(NOT IS_DIRECTORY ${RESOURCE_DIR})
+#    set(${NAME}_FOUND FALSE PARENT_SCOPE)
+#    return()
+#  endif(NOT IS_DIRECTORY ${RESOURCE_DIR})
+#
+#  set(${NAME}_DIR ${RESOURCE_DIR} PARENT_SCOPE)
+#  set(${NAME}_FOUND TRUE PARENT_SCOPE)
+#endfunction(nncc_find_resource)
+#
+####
+#### CMake configuration
+####
+#if(NOT CMAKE_BUILD_TYPE)
+#  set(CMAKE_BUILD_TYPE "Debug" CACHE STRING "Type of build" FORCE)
+#endif(NOT CMAKE_BUILD_TYPE)
+#message(STATUS "Use '${CMAKE_BUILD_TYPE}' configuration")
+#
+## Prefer -pthread to -lpthread for find_package(Threads ...)
+##
+## std::thread code compiled only with -lpthread emits the following runtime error (on GCC 4.8.4)
+##
+##   terminate called after throwing an instance of 'std::system_error'
+##     what():  Enable multithreading to use std::thread: Operation not permitted
+##
+#set(THREADS_PREFER_PTHREAD_FLAG TRUE)
+#
+####
+#### Configuration
+####
+#option(DOWNLOAD_PROTOBUF "Download Protocol Buffer source" ON)
+#option(BUILD_PROTOBUF "Locally build Protocol Buffer from the downloaded source" ON)
+#option(DOWNLOAD_EIGEN "Download Eigen source" ON)
+#option(DOWNLOAD_FARMHASH "Download farmhash source" ON)
+#option(DOWNLOAD_GEMMLOWP "Download GEMM low precesion library source" ON)
+#option(DOWNLOAD_RUY "Download ruy source" ON)
+#option(DOWNLOAD_NEON2SSE "Download NEON2SSE library source" ON)
+#option(DOWNLOAD_GFLAGS "Download GFlags source" OFF)
+#option(DOWNLOAD_FLATBUFFERS "Download FlatBuffers source" ON)
+#option(BUILD_FLATBUFFERS "Locally build Flatbuffers from the downloaded source" ON)
+#option(DOWNLOAD_TENSORFLOW "Download TensorFlow source" ON)
+#option(DOWNLOAD_CAFFE "Download Caffe source" ON)
+#option(DOWNLOAD_PYTORCH "Download Pytorch source" ON)
+#option(DOWNLOAD_ONNX "Download ONNX source" ON)
+#option(DOWNLOAD_ABSEIL "Download Abseil-cpp source" ON)
+#option(DOWNLOAD_OPENCL_HEADERS "Download OpenCl Header source" ON)
+#option(DOWNLOAD_PYBIND11 "Download Pybind11 source" ON)
+#
+#option(DOWNLOAD_GTEST "Download Google Test source" ON)
+#option(BUILD_GTEST "Build Google Test from the downloaded source" ON)
+#option(DOWNLOAD_HDF5 "Download HDF5 source" ON)
+#option(BUILD_HDF5 "Build HDF5 from the downloaded source" ON)
+#
+#nnas_find_package(GTest QUIET)
+#
+#option(ENABLE_TEST "Build Tests using Google Test" ${GTest_FOUND})
+#
+#if(${ENABLE_TEST} AND NOT ${GTest_FOUND})
+#  message(FATAL_ERROR "Google Test is required to enable test")
+#endif(${ENABLE_TEST} AND NOT ${GTest_FOUND})
+#
+#option(ENABLE_COVERAGE "Build for coverage test" OFF)
+#if(${ENABLE_COVERAGE} AND NOT ${ENABLE_TEST})
+#  message(FATAL_ERROR "Test should be enabled to measure test coverage")
+#endif(${ENABLE_COVERAGE} AND NOT ${ENABLE_TEST})
+#
+#if(${ENABLE_TEST})
+#  include(CTest)
+#endif(${ENABLE_TEST})
+#
+#option(ENABLE_STRICT_BUILD "Treat warning as error" OFF)
+#
+## This option might be turned ON for Windows native build.
+## Check our ProtobufConfig.cmake for its usage.
+#option(USE_PROTOBUF_LEGACY_IMPORT "Use legacy MODULE mode import rather than CONFIG mode" OFF)
+#
+####
+#### Target
+####
 add_library(nncc_common INTERFACE)
-if(ENABLE_STRICT_BUILD)
-  # TODO Remove -Wno-reoder
-  target_compile_options(nncc_common INTERFACE -Werror -Wall -Wextra -Wno-reorder)
-endif(ENABLE_STRICT_BUILD)
-
+#if(ENABLE_STRICT_BUILD)
+#  # TODO Remove -Wno-reoder
+#  target_compile_options(nncc_common INTERFACE -Werror -Wall -Wextra -Wno-reorder)
+#endif(ENABLE_STRICT_BUILD)
+#
 add_library(nncc_coverage INTERFACE)
-if(ENABLE_COVERAGE)
-  target_compile_options(nncc_coverage INTERFACE -g -O0 -fprofile-arcs -ftest-coverage)
-  target_link_libraries(nncc_coverage INTERFACE gcov)
-endif(ENABLE_COVERAGE)
-
-###
-### Function
-###
-# TODO Remove this nnas_include
-nnas_include(OptionalTargetTools)
-nnas_include(AddSubdirectories)
-
-add_subdirectory("${NNAS_PROJECT_SOURCE_DIR}/compiler" "${CMAKE_BINARY_DIR}/compiler")
+#if(ENABLE_COVERAGE)
+#  target_compile_options(nncc_coverage INTERFACE -g -O0 -fprofile-arcs -ftest-coverage)
+#  target_link_libraries(nncc_coverage INTERFACE gcov)
+#endif(ENABLE_COVERAGE)
+#
+####
+#### Function
+####
+## TODO Remove this nnas_include
+#nnas_include(OptionalTargetTools)
+#nnas_include(AddSubdirectories)
+#
+#add_subdirectory("${NNAS_PROJECT_SOURCE_DIR}/compiler" "${CMAKE_BINARY_DIR}/compiler")
